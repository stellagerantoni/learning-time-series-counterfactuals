{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/learning-time-series-counterfactuals/blob/main/multivariate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIiG4khZBur-",
        "outputId": "edb01f17-7638-4f8a-bfd4-fd6fd1bd4966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'learning-time-series-counterfactuals'...\n",
            "remote: Enumerating objects: 318, done.\u001b[K\n",
            "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 318 (delta 126), reused 24 (delta 24), pack-reused 138\u001b[K\n",
            "Receiving objects: 100% (318/318), 4.39 MiB | 6.30 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "/content/learning-time-series-counterfactuals\n"
          ]
        }
      ],
      "source": [
        " ! git clone https://github.com/stellagerantoni/learning-time-series-counterfactuals\n",
        " %cd learning-time-series-counterfactuals/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon[all_extras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89L3kts7CCan",
        "outputId": "649abd5e-516a-405c-ac60-58b1520bab41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aeon[all_extras]\n",
            "  Downloading aeon-0.4.0-py3-none-any.whl (39.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1.0)\n",
            "Collecting deprecated>=1.2.13 (from aeon[all_extras])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.56.4)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.3.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.2.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.8.1)\n",
            "Collecting filterpy>=1.4.5 (from aeon[all_extras])\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.9.0)\n",
            "Collecting hmmlearn>=0.2.7 (from aeon[all_extras])\n",
            "  Downloading hmmlearn-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonts>=0.12.4 (from aeon[all_extras])\n",
            "  Downloading gluonts-0.13.4-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.1/812.1 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-self-attention (from aeon[all_extras])\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kotsu>=0.3.1 (from aeon[all_extras])\n",
            "  Downloading kotsu-0.3.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.7.1)\n",
            "Collecting mne (from aeon[all_extras])\n",
            "  Downloading mne-1.5.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pmdarima<3.0.0,>=1.8.0 (from aeon[all_extras])\n",
            "  Downloading pmdarima-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prophet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.4)\n",
            "Collecting pyod>=0.8.0 (from aeon[all_extras])\n",
            "  Downloading pyod-1.1.0.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.4/153.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-posthocs>=0.6.5 (from aeon[all_extras])\n",
            "  Downloading scikit_posthocs-0.7.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.12.2)\n",
            "Collecting statsforecast>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading statsforecast-1.6.0-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.9/110.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.14.0)\n",
            "Requirement already satisfied: stumpy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.12.0)\n",
            "Collecting tbats>=1.1.0 (from aeon[all_extras])\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow<2.13.0 (from aeon[all_extras])\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability<0.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.20.1)\n",
            "Collecting tsfresh>=0.20.0 (from aeon[all_extras])\n",
            "  Downloading tsfresh-0.20.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tslearn<0.6.0,>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading tslearn-0.5.3.2-py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.2/358.2 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.7.0)\n",
            "Collecting mlflow<2.4.0 (from aeon[all_extras])\n",
            "  Downloading mlflow-2.3.2-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting esig<0.9.8.3,>=0.9.7 (from aeon[all_extras])\n",
            "  Downloading esig-0.9.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon[all_extras]) (1.15.0)\n",
            "Requirement already satisfied: pydantic~=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (1.10.12)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.66.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (8.1.7)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading GitPython-3.1.35-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.31.0)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading alembic-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.2.5)\n",
            "Collecting querystring-parser<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.0.20)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.4.4)\n",
            "Collecting gunicorn<21 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (0.29.36)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (2.0.4)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.32)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.8.0->aeon[all_extras]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3.0,>=1.0.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (from statsforecast>=0.5.2->aeon[all_extras]) (0.17.3)\n",
            "Collecting fugue>=0.8.1 (from statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-0.8.6-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.0/275.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.1->aeon[all_extras]) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.57.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.14)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow<2.13.0->aeon[all_extras])\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (3.3.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow<2.13.0->aeon[all_extras])\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow<2.13.0->aeon[all_extras])\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.3.0)\n",
            "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.13->aeon[all_extras])\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.33.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (0.1.8)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.20.0->aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (1.7.0)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13.0->aeon[all_extras]) (0.41.2)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->prophet>=1.1.0->aeon[all_extras]) (0.5.12)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (0.9.0)\n",
            "Collecting urllib3 (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras])\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (6.3.2)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow<2.4.0->aeon[all_extras]) (1.6.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.3.7)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.1.2)\n",
            "Collecting triad>=0.9.1 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading triad-0.9.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting qpd>=0.4.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading qpd-0.4.4-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugue-sql-antlr>=0.1.6 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-sql-antlr-0.1.6.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.6/154.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sqlglot (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading sqlglot-18.3.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.4/301.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow<2.4.0->aeon[all_extras]) (3.16.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow<2.4.0->aeon[all_extras]) (2.1.3)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet>=1.1.0->aeon[all_extras]) (4.1.4)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->aeon[all_extras]) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.4.0->aeon[all_extras]) (2.0.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.7.1)\n",
            "Collecting antlr4-python3-runtime<4.12,>=4.11.1 (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading antlr4_python3_runtime-4.11.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.3.1)\n",
            "Collecting fs (from triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.5.0)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (1.4.4)\n",
            "Building wheels for collected packages: filterpy, pyod, keras-self-attention, databricks-cli, fugue-sql-antlr\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=774f214f1192e2be0f158bc3a60be75ea74ff5c32005173a0bb76ed00479f134\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.1.0-py3-none-any.whl size=185329 sha256=97b16dc20bf6a136442278c4efed131bc028b98790f075d5a1d76f6f33639218\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/8e/e2/e932956b10b843eb6be9eefa70b5c1bee7b561be14c423b136\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=a04da60b903a77fc70d9e1b441f2cf960cfd4976d94d84cc36680c77cdb88500\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143855 sha256=8529729326450695cdfc810b35faa82b5e5fa588e6005402129a85a323ba6a97\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/63/93/5402c1a09c1868a59d0b05013484e07af97a9d7b3dbd5bd39a\n",
            "  Building wheel for fugue-sql-antlr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fugue-sql-antlr: filename=fugue_sql_antlr-0.1.6-py3-none-any.whl size=158045 sha256=0fb035c83426d7a861e6021fa96892a0d4bd82d3a0f49c9f4eaaee4bb845bc80\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/54/a1/b294b8b33c6107946b5720b3acb1fce07b97bbbc9677a501ce\n",
            "Successfully built filterpy pyod keras-self-attention databricks-cli fugue-sql-antlr\n",
            "Installing collected packages: antlr4-python3-runtime, wrapt, urllib3, tensorflow-estimator, sqlglot, smmap, querystring-parser, Mako, keras-self-attention, keras, gunicorn, fs, esig, gitdb, deprecated, alembic, tslearn, triad, pyod, kotsu, hmmlearn, gluonts, gitpython, filterpy, docker, databricks-cli, aeon, scikit-posthocs, pmdarima, mne, mlflow, fugue-sql-antlr, adagio, tsfresh, tensorboard, tbats, qpd, tensorflow, fugue, statsforecast\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed Mako-1.2.4 adagio-0.2.4 aeon-0.4.0 alembic-1.12.0 antlr4-python3-runtime-4.11.1 databricks-cli-0.17.7 deprecated-1.2.14 docker-6.1.3 esig-0.9.8.2 filterpy-1.4.5 fs-2.4.16 fugue-0.8.6 fugue-sql-antlr-0.1.6 gitdb-4.0.10 gitpython-3.1.35 gluonts-0.13.4 gunicorn-20.1.0 hmmlearn-0.3.0 keras-2.12.0 keras-self-attention-0.51.0 kotsu-0.3.3 mlflow-2.3.2 mne-1.5.1 pmdarima-2.0.3 pyod-1.1.0 qpd-0.4.4 querystring-parser-1.2.4 scikit-posthocs-0.7.0 smmap-5.0.0 sqlglot-18.3.0 statsforecast-1.6.0 tbats-1.1.3 tensorboard-2.12.3 tensorflow-2.12.1 tensorflow-estimator-2.12.0 triad-0.9.1 tsfresh-0.20.1 tslearn-0.5.3.2 urllib3-1.26.16 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "\n",
        "from _composite import ModifiedLatentCF\n",
        "%cd src\n",
        "from _vanilla import LatentCF\n",
        "from help_functions import (ResultWriter, conditional_pad, evaluate,\n",
        "                            find_best_lr, plot_graphs,\n",
        "                            reset_seeds, time_series_normalize,\n",
        "                            time_series_revert, upsample_minority,\n",
        "                            validity_score)\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "c5f49371-449f-4f46-a78b-f6051c0c07f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/learning-time-series-counterfactuals/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FUNCTIONS**"
      ],
      "metadata": {
        "id": "IHBw9E_4Zm5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y, meta_data = load_classification(dataset)\n",
        "  if dataset == 'Heartbeat':\n",
        "    pos = 'normal'\n",
        "    neg = 'abnormal'\n",
        "    X = X.transpose(0,2,1)\n",
        "  if dataset == 'SelfRegulationSCP1':\n",
        "    pos = 'positivity'\n",
        "    neg = 'negativity'\n",
        "    X = X.transpose(0,2,1)\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y,pos_label, neg_label"
      ],
      "metadata": {
        "id": "QsJJktx22dXs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normalize_multivariate(data, n_timesteps, n_features, scaler=None):\n",
        "\n",
        "    # Then reshape data to have timesteps as rows for normalization\n",
        "    data_reshaped = data.reshape(-1, n_features)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaler.fit(data_reshaped)\n",
        "\n",
        "    normalized = scaler.transform(data_reshaped)\n",
        "\n",
        "    # Return data reshaped\n",
        "    data = normalized.reshape(-1, n_timesteps, n_features)\n",
        "    return data, scaler\n",
        "\n",
        "def conditional_pad_multivariate(X):\n",
        "    num_timesteps = X.shape[1]\n",
        "\n",
        "    if num_timesteps % 4 != 0:\n",
        "        next_num = (int(num_timesteps / 4) + 1) * 4\n",
        "        padding_size = next_num - num_timesteps\n",
        "        X_padded = np.pad(\n",
        "            X, pad_width=((0, 0), (0, padding_size), (0, 0))\n",
        "        )\n",
        "\n",
        "        return X_padded, padding_size\n",
        "\n",
        "    return X, 0\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "MQmQpShMijLz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**\n",
        "datasets available : 'Heartbeat', 'SelfRegulationSCP1'"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,pos_label, neg_label = load_dataset('SelfRegulationSCP1')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "0a57b541-87a2-4bf5-f704-c34d0d04a509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (561, 896, 6)\n",
            " Shape of y =  (561,)\n",
            " Meta data =  {'problemname': 'selfregulationscp1', 'timestamps': False, 'missing': False, 'univariate': False, 'equallength': True, 'classlabel': True, 'targetlabel': False, 'class_values': ['negativity', 'positivity']}\n",
            "\n",
            " X[:1] = \n",
            "[[[23.   19.03 32.19 43.66 30.72 39.09]\n",
            "  [21.66 19.19 36.81 41.22 31.81 39.53]\n",
            "  [20.84 21.5  40.16 39.69 31.69 40.59]\n",
            "  ...\n",
            "  [21.84 27.03 32.5  50.06 44.69 44.03]\n",
            "  [22.62 26.78 34.53 50.84 43.69 44.75]\n",
            "  [21.5  28.94 37.12 50.75 41.88 46.19]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsample the minority class\n",
        "\n",
        "pos_counts = pd.value_counts(y_train)[pos_label]\n",
        "neg_counts = pd.value_counts(y_train)[neg_label]\n",
        "print(f\"negative_count = {neg_counts}, positive_count = {pos_counts}\")\n",
        "\n",
        "if pos_counts!=neg_counts:\n",
        "  X_train, y_train = upsample_minority(X_train, y_train, pos_label=pos_label, neg_label=neg_label)\n",
        "  print(f\"Data upsampling performed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n",
        "else:\n",
        "   print(f\"Data upsampling not needed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "ea597acd-7d76-4f05-9a13-26d5cb6d75b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative_count = 225, positive_count = 223\n",
            "Data upsampling performed, current distribution of y: \n",
            "0    225\n",
            "1    225\n",
            "dtype: int64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "c8693054-51ad-40d8-d6e5-20e47194eea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=896, padded #timesteps=896.\n",
            "\n",
            "min value = -73.84, max value = 175.06\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (450, 896, 6)\n",
            "X_train_processed_padded.shape = (450, 896, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_classes = y_train\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))"
      ],
      "metadata": {
        "id": "XHvhyShuvFuO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=1\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    inputs = keras.Input(shape=(n_features, n_timesteps), dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "8v2HDOL6WUbA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2\n",
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=1\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    inputs = keras.Input(shape=( n_timesteps,n_features), dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "def LSTMFCNClassifier(n_timesteps, n_features, n_output=2, n_LSTM_cells=8, regularization_rate = 0.001):\n",
        "    # https://github.com/titu1994/LSTM-FCN/blob/master/hyperparameter_search.py\n",
        "    inputs = keras.Input(shape=( n_features,n_timesteps), dtype=\"float32\")\n",
        "\n",
        "    x = keras.layers.LSTM(units=n_LSTM_cells, kernel_regularizer=l2(regularization_rate))(inputs)\n",
        "    x = keras.layers.Dropout(rate=0.8)(x)\n",
        "\n",
        "    y = keras.layers.Permute((2, 1))(inputs)\n",
        "    y = keras.layers.Conv1D(64, 8, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(128, 5, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(64, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    x = keras.layers.concatenate([x, y])\n",
        "\n",
        "    outputs = keras.layers.Dense(n_output, activation=\"softmax\", kernel_regularizer=l2(regularization_rate))(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def ClassifierLSTM(n_timesteps, n_features, extra_lstm_layer=True, n_output=1):\n",
        "    # Define the model structure - only LSTM layers\n",
        "    # https://www.kaggle.com/szaitseff/classification-of-time-series-with-lstm-rnn\n",
        "    inputs = keras.Input(shape=(n_features,n_timesteps), dtype=\"float32\")\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(\n",
        "            inputs\n",
        "        )  # set return_sequences true to feed next LSTM layer\n",
        "    else:\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(\n",
        "            inputs\n",
        "        )  # set return_sequences false to feed dense layer directly\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    # x = keras.layers.LSTM(32, activation='tanh', return_sequences=True)(x)\n",
        "    # x = keras.layers.BatchNormalization()(x)\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(16, activation=\"tanh\", return_sequences=False)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier2 = keras.Model(inputs, outputs)\n",
        "\n",
        "    return classifier2\n",
        "\n",
        "def Classifier_FCN(input_shape, nb_classes):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation=\"relu\")(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation(\"relu\")(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation(\"relu\")(conv3)\n",
        "\n",
        "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(nb_classes, activation=\"softmax\")(gap_layer)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sD3qinpSEkWK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)"
      ],
      "metadata": {
        "id": "ehoPG4lPzXl5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_lstmcells = 8\n",
        "\n",
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ## 2.0 LSTM-FCN classifier\n",
        "###############################################\n",
        "# ### LSTM-FCN classifier\n",
        "classifier = LSTMFCNClassifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, n_LSTM_cells=n_lstmcells\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "0a3953a9-90ed-4e70-e844-a4725be142de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "15/15 [==============================] - 8s 89ms/step - loss: 1.0769 - accuracy: 0.7711 - val_loss: 1.2399 - val_accuracy: 0.5044\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 1.0126 - accuracy: 0.7667 - val_loss: 1.2158 - val_accuracy: 0.5044\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.9620 - accuracy: 0.7911 - val_loss: 1.1884 - val_accuracy: 0.5044\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.9180 - accuracy: 0.7711 - val_loss: 1.1736 - val_accuracy: 0.5044\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.8958 - accuracy: 0.7533 - val_loss: 1.0916 - val_accuracy: 0.5221\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.8629 - accuracy: 0.7644 - val_loss: 1.0618 - val_accuracy: 0.5310\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.8036 - accuracy: 0.7933 - val_loss: 1.0070 - val_accuracy: 0.5487\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.7832 - accuracy: 0.7822 - val_loss: 0.9714 - val_accuracy: 0.5664\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.7624 - accuracy: 0.8067 - val_loss: 0.9302 - val_accuracy: 0.5664\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.7436 - accuracy: 0.7933 - val_loss: 0.8638 - val_accuracy: 0.7168\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.7226 - accuracy: 0.7956 - val_loss: 0.8139 - val_accuracy: 0.7965\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.7117 - accuracy: 0.7800 - val_loss: 0.7876 - val_accuracy: 0.7257\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6869 - accuracy: 0.7933 - val_loss: 0.7811 - val_accuracy: 0.7345\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6595 - accuracy: 0.8022 - val_loss: 0.7785 - val_accuracy: 0.7168\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6550 - accuracy: 0.7867 - val_loss: 0.7779 - val_accuracy: 0.6637\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6452 - accuracy: 0.7911 - val_loss: 0.8257 - val_accuracy: 0.5310\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.6387 - accuracy: 0.8133 - val_loss: 0.8801 - val_accuracy: 0.5398\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6403 - accuracy: 0.7800 - val_loss: 0.8276 - val_accuracy: 0.5752\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6054 - accuracy: 0.8111 - val_loss: 0.9124 - val_accuracy: 0.5398\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5954 - accuracy: 0.8089 - val_loss: 0.6809 - val_accuracy: 0.7522\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5728 - accuracy: 0.8156 - val_loss: 0.7096 - val_accuracy: 0.6991\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.5843 - accuracy: 0.8022 - val_loss: 0.7472 - val_accuracy: 0.6549\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5686 - accuracy: 0.8244 - val_loss: 0.9840 - val_accuracy: 0.4956\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5766 - accuracy: 0.8044 - val_loss: 0.8075 - val_accuracy: 0.6637\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.6059 - accuracy: 0.7800 - val_loss: 0.7082 - val_accuracy: 0.6991\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5706 - accuracy: 0.7978 - val_loss: 0.6423 - val_accuracy: 0.7611\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5659 - accuracy: 0.7956 - val_loss: 0.6964 - val_accuracy: 0.7168\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5514 - accuracy: 0.8000 - val_loss: 0.7589 - val_accuracy: 0.6637\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5759 - accuracy: 0.8089 - val_loss: 0.6639 - val_accuracy: 0.7522\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5390 - accuracy: 0.8000 - val_loss: 1.0442 - val_accuracy: 0.5575\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5485 - accuracy: 0.7867 - val_loss: 0.6611 - val_accuracy: 0.7522\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5263 - accuracy: 0.8222 - val_loss: 0.7213 - val_accuracy: 0.7080\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5269 - accuracy: 0.8178 - val_loss: 0.8399 - val_accuracy: 0.6549\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5243 - accuracy: 0.8067 - val_loss: 1.0658 - val_accuracy: 0.5221\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.5317 - accuracy: 0.8067 - val_loss: 0.6738 - val_accuracy: 0.8230\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5150 - accuracy: 0.8044 - val_loss: 0.8713 - val_accuracy: 0.4956\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.5386 - accuracy: 0.8156 - val_loss: 0.6842 - val_accuracy: 0.7080\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5133 - accuracy: 0.8156 - val_loss: 1.0331 - val_accuracy: 0.7257\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5182 - accuracy: 0.7911 - val_loss: 0.5810 - val_accuracy: 0.8319\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5239 - accuracy: 0.8200 - val_loss: 0.6166 - val_accuracy: 0.7434\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5379 - accuracy: 0.7956 - val_loss: 0.7108 - val_accuracy: 0.6372\n",
            "Epoch 42/150\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.5732 - accuracy: 0.7489 - val_loss: 0.6347 - val_accuracy: 0.7257\n",
            "Epoch 43/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5476 - accuracy: 0.8333 - val_loss: 0.5766 - val_accuracy: 0.8319\n",
            "Epoch 44/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.5292 - accuracy: 0.8089 - val_loss: 3.3051 - val_accuracy: 0.4956\n",
            "Epoch 45/150\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.5929 - accuracy: 0.7356 - val_loss: 4.5064 - val_accuracy: 0.5310\n",
            "Epoch 46/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.5455 - accuracy: 0.7911 - val_loss: 1.0611 - val_accuracy: 0.5664\n",
            "Epoch 47/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.5182 - accuracy: 0.8133 - val_loss: 2.0144 - val_accuracy: 0.5575\n",
            "Epoch 48/150\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.5082 - accuracy: 0.8111 - val_loss: 1.6603 - val_accuracy: 0.4956\n",
            "Epoch 49/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.5021 - accuracy: 0.8111 - val_loss: 1.8556 - val_accuracy: 0.4956\n",
            "Epoch 50/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5114 - accuracy: 0.8178 - val_loss: 0.8053 - val_accuracy: 0.8053\n",
            "Epoch 51/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.4897 - accuracy: 0.8133 - val_loss: 0.8745 - val_accuracy: 0.6460\n",
            "Epoch 52/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4845 - accuracy: 0.8156 - val_loss: 0.7177 - val_accuracy: 0.7257\n",
            "Epoch 53/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.4786 - accuracy: 0.8311 - val_loss: 1.3017 - val_accuracy: 0.7345\n",
            "Epoch 54/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.5065 - accuracy: 0.8133 - val_loss: 0.5945 - val_accuracy: 0.7788\n",
            "Epoch 55/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4908 - accuracy: 0.8156 - val_loss: 0.9077 - val_accuracy: 0.4956\n",
            "Epoch 56/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4685 - accuracy: 0.8222 - val_loss: 0.5977 - val_accuracy: 0.7611\n",
            "Epoch 57/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4876 - accuracy: 0.8333 - val_loss: 4.7427 - val_accuracy: 0.4956\n",
            "Epoch 58/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.5640 - accuracy: 0.7756 - val_loss: 1.5356 - val_accuracy: 0.5487\n",
            "Epoch 59/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.5523 - accuracy: 0.7844 - val_loss: 4.6788 - val_accuracy: 0.4956\n",
            "Epoch 60/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5318 - accuracy: 0.8067 - val_loss: 1.6285 - val_accuracy: 0.4956\n",
            "Epoch 61/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.5161 - accuracy: 0.8133 - val_loss: 0.8863 - val_accuracy: 0.6549\n",
            "Epoch 62/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4887 - accuracy: 0.8111 - val_loss: 5.8917 - val_accuracy: 0.4956\n",
            "Epoch 63/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.5188 - accuracy: 0.7889 - val_loss: 2.0697 - val_accuracy: 0.5752\n",
            "Epoch 64/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.4817 - accuracy: 0.8178 - val_loss: 3.9898 - val_accuracy: 0.4956\n",
            "Epoch 65/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.4907 - accuracy: 0.8444 - val_loss: 2.3465 - val_accuracy: 0.5752\n",
            "Epoch 66/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.5016 - accuracy: 0.7978 - val_loss: 2.6487 - val_accuracy: 0.5575\n",
            "Epoch 67/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.4696 - accuracy: 0.8244 - val_loss: 1.0309 - val_accuracy: 0.6549\n",
            "Epoch 68/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.4549 - accuracy: 0.8467 - val_loss: 1.2454 - val_accuracy: 0.5841\n",
            "Epoch 69/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.4747 - accuracy: 0.8178 - val_loss: 2.0828 - val_accuracy: 0.5133\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.8309837092731829.\n",
            "Confusion matrix: \n",
            "          Pred:pos  Pred:neg\n",
            "True:pos        41        15\n",
            "True:neg         4        53.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### LSTM classifier\n",
        "classifier = ClassifierLSTM(\n",
        "    n_timesteps_padded, n_features,extra_lstm_layer=False, n_output=2\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=42,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")"
      ],
      "metadata": {
        "id": "fvEe9-iRwX-h",
        "outputId": "be491f21-adad-4df5-f837-ad6ad354fe80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM classifier:\n",
            "Epoch 1/150\n",
            "11/11 - 5s - loss: 0.4709 - accuracy: 0.7933 - val_loss: 0.7260 - val_accuracy: 0.5044 - 5s/epoch - 429ms/step\n",
            "Epoch 2/150\n",
            "11/11 - 0s - loss: 0.3649 - accuracy: 0.8489 - val_loss: 0.6449 - val_accuracy: 0.5752 - 133ms/epoch - 12ms/step\n",
            "Epoch 3/150\n",
            "11/11 - 0s - loss: 0.3523 - accuracy: 0.8600 - val_loss: 0.6244 - val_accuracy: 0.5841 - 143ms/epoch - 13ms/step\n",
            "Epoch 4/150\n",
            "11/11 - 0s - loss: 0.3306 - accuracy: 0.8622 - val_loss: 0.6391 - val_accuracy: 0.5487 - 135ms/epoch - 12ms/step\n",
            "Epoch 5/150\n",
            "11/11 - 0s - loss: 0.3197 - accuracy: 0.8556 - val_loss: 0.6086 - val_accuracy: 0.5929 - 131ms/epoch - 12ms/step\n",
            "Epoch 6/150\n",
            "11/11 - 0s - loss: 0.2936 - accuracy: 0.8667 - val_loss: 0.5687 - val_accuracy: 0.7168 - 126ms/epoch - 11ms/step\n",
            "Epoch 7/150\n",
            "11/11 - 0s - loss: 0.2735 - accuracy: 0.8911 - val_loss: 0.5807 - val_accuracy: 0.6726 - 120ms/epoch - 11ms/step\n",
            "Epoch 8/150\n",
            "11/11 - 0s - loss: 0.2790 - accuracy: 0.8889 - val_loss: 0.5920 - val_accuracy: 0.7699 - 128ms/epoch - 12ms/step\n",
            "Epoch 9/150\n",
            "11/11 - 0s - loss: 0.2619 - accuracy: 0.8911 - val_loss: 0.6439 - val_accuracy: 0.5133 - 117ms/epoch - 11ms/step\n",
            "Epoch 10/150\n",
            "11/11 - 0s - loss: 0.2832 - accuracy: 0.8733 - val_loss: 0.5181 - val_accuracy: 0.8230 - 137ms/epoch - 12ms/step\n",
            "Epoch 11/150\n",
            "11/11 - 0s - loss: 0.2505 - accuracy: 0.8911 - val_loss: 0.5853 - val_accuracy: 0.5929 - 130ms/epoch - 12ms/step\n",
            "Epoch 12/150\n",
            "11/11 - 0s - loss: 0.2405 - accuracy: 0.9044 - val_loss: 0.5082 - val_accuracy: 0.7876 - 127ms/epoch - 12ms/step\n",
            "Epoch 13/150\n",
            "11/11 - 0s - loss: 0.2225 - accuracy: 0.9200 - val_loss: 0.5547 - val_accuracy: 0.6460 - 124ms/epoch - 11ms/step\n",
            "Epoch 14/150\n",
            "11/11 - 0s - loss: 0.2123 - accuracy: 0.9156 - val_loss: 0.5328 - val_accuracy: 0.7080 - 138ms/epoch - 13ms/step\n",
            "Epoch 15/150\n",
            "11/11 - 0s - loss: 0.2112 - accuracy: 0.9111 - val_loss: 0.4828 - val_accuracy: 0.8584 - 119ms/epoch - 11ms/step\n",
            "Epoch 16/150\n",
            "11/11 - 0s - loss: 0.2314 - accuracy: 0.9156 - val_loss: 0.4366 - val_accuracy: 0.8496 - 97ms/epoch - 9ms/step\n",
            "Epoch 17/150\n",
            "11/11 - 0s - loss: 0.2201 - accuracy: 0.9111 - val_loss: 0.4775 - val_accuracy: 0.8673 - 100ms/epoch - 9ms/step\n",
            "Epoch 18/150\n",
            "11/11 - 0s - loss: 0.2085 - accuracy: 0.9289 - val_loss: 0.4464 - val_accuracy: 0.8850 - 92ms/epoch - 8ms/step\n",
            "Epoch 19/150\n",
            "11/11 - 0s - loss: 0.2124 - accuracy: 0.9222 - val_loss: 0.4513 - val_accuracy: 0.8319 - 87ms/epoch - 8ms/step\n",
            "Epoch 20/150\n",
            "11/11 - 0s - loss: 0.1822 - accuracy: 0.9356 - val_loss: 0.6713 - val_accuracy: 0.5575 - 90ms/epoch - 8ms/step\n",
            "Epoch 21/150\n",
            "11/11 - 0s - loss: 0.1776 - accuracy: 0.9356 - val_loss: 0.5592 - val_accuracy: 0.6637 - 83ms/epoch - 8ms/step\n",
            "Epoch 22/150\n",
            "11/11 - 0s - loss: 0.1960 - accuracy: 0.9200 - val_loss: 0.6018 - val_accuracy: 0.6018 - 101ms/epoch - 9ms/step\n",
            "Epoch 23/150\n",
            "11/11 - 0s - loss: 0.2022 - accuracy: 0.9289 - val_loss: 0.4437 - val_accuracy: 0.8230 - 87ms/epoch - 8ms/step\n",
            "Epoch 24/150\n",
            "11/11 - 0s - loss: 0.1906 - accuracy: 0.9222 - val_loss: 0.5627 - val_accuracy: 0.7080 - 83ms/epoch - 8ms/step\n",
            "Epoch 25/150\n",
            "11/11 - 0s - loss: 0.1924 - accuracy: 0.9156 - val_loss: 1.5943 - val_accuracy: 0.4956 - 81ms/epoch - 7ms/step\n",
            "Epoch 26/150\n",
            "11/11 - 0s - loss: 0.1705 - accuracy: 0.9467 - val_loss: 1.5641 - val_accuracy: 0.4956 - 92ms/epoch - 8ms/step\n",
            "Epoch 27/150\n",
            "11/11 - 0s - loss: 0.1846 - accuracy: 0.9311 - val_loss: 0.3825 - val_accuracy: 0.8584 - 83ms/epoch - 8ms/step\n",
            "Epoch 28/150\n",
            "11/11 - 0s - loss: 0.1505 - accuracy: 0.9578 - val_loss: 0.5813 - val_accuracy: 0.6637 - 122ms/epoch - 11ms/step\n",
            "Epoch 29/150\n",
            "11/11 - 0s - loss: 0.1482 - accuracy: 0.9467 - val_loss: 0.5787 - val_accuracy: 0.6460 - 85ms/epoch - 8ms/step\n",
            "Epoch 30/150\n",
            "11/11 - 0s - loss: 0.1621 - accuracy: 0.9422 - val_loss: 2.3580 - val_accuracy: 0.4956 - 79ms/epoch - 7ms/step\n",
            "Epoch 31/150\n",
            "11/11 - 0s - loss: 0.1567 - accuracy: 0.9378 - val_loss: 0.4371 - val_accuracy: 0.7965 - 80ms/epoch - 7ms/step\n",
            "Epoch 32/150\n",
            "11/11 - 0s - loss: 0.1682 - accuracy: 0.9267 - val_loss: 0.6862 - val_accuracy: 0.6195 - 79ms/epoch - 7ms/step\n",
            "Epoch 33/150\n",
            "11/11 - 0s - loss: 0.1392 - accuracy: 0.9578 - val_loss: 1.2358 - val_accuracy: 0.5133 - 96ms/epoch - 9ms/step\n",
            "Epoch 34/150\n",
            "11/11 - 0s - loss: 0.2228 - accuracy: 0.9067 - val_loss: 1.0601 - val_accuracy: 0.5929 - 99ms/epoch - 9ms/step\n",
            "Epoch 35/150\n",
            "11/11 - 0s - loss: 0.1957 - accuracy: 0.9422 - val_loss: 0.7508 - val_accuracy: 0.6814 - 83ms/epoch - 8ms/step\n",
            "Epoch 36/150\n",
            "11/11 - 0s - loss: 0.1878 - accuracy: 0.9422 - val_loss: 1.0881 - val_accuracy: 0.5398 - 98ms/epoch - 9ms/step\n",
            "Epoch 37/150\n",
            "11/11 - 0s - loss: 0.1957 - accuracy: 0.9178 - val_loss: 0.7998 - val_accuracy: 0.6106 - 80ms/epoch - 7ms/step\n",
            "Epoch 38/150\n",
            "11/11 - 0s - loss: 0.1644 - accuracy: 0.9356 - val_loss: 1.9422 - val_accuracy: 0.4956 - 78ms/epoch - 7ms/step\n",
            "Epoch 39/150\n",
            "11/11 - 0s - loss: 0.1552 - accuracy: 0.9422 - val_loss: 0.9841 - val_accuracy: 0.5752 - 91ms/epoch - 8ms/step\n",
            "Epoch 40/150\n",
            "11/11 - 0s - loss: 0.1667 - accuracy: 0.9422 - val_loss: 3.4737 - val_accuracy: 0.4956 - 93ms/epoch - 8ms/step\n",
            "Epoch 41/150\n",
            "11/11 - 0s - loss: 0.1527 - accuracy: 0.9489 - val_loss: 1.1192 - val_accuracy: 0.5575 - 99ms/epoch - 9ms/step\n",
            "Epoch 42/150\n",
            "11/11 - 0s - loss: 0.1884 - accuracy: 0.9267 - val_loss: 0.9755 - val_accuracy: 0.7611 - 102ms/epoch - 9ms/step\n",
            "Epoch 43/150\n",
            "11/11 - 0s - loss: 0.1633 - accuracy: 0.9489 - val_loss: 0.7611 - val_accuracy: 0.7611 - 94ms/epoch - 9ms/step\n",
            "Epoch 44/150\n",
            "11/11 - 0s - loss: 0.1638 - accuracy: 0.9444 - val_loss: 1.0247 - val_accuracy: 0.6106 - 104ms/epoch - 9ms/step\n",
            "Epoch 45/150\n",
            "11/11 - 0s - loss: 0.1231 - accuracy: 0.9578 - val_loss: 1.0311 - val_accuracy: 0.5664 - 122ms/epoch - 11ms/step\n",
            "Epoch 46/150\n",
            "11/11 - 0s - loss: 0.1417 - accuracy: 0.9400 - val_loss: 1.2211 - val_accuracy: 0.5487 - 103ms/epoch - 9ms/step\n",
            "Epoch 47/150\n",
            "11/11 - 0s - loss: 0.1233 - accuracy: 0.9556 - val_loss: 0.9983 - val_accuracy: 0.5841 - 91ms/epoch - 8ms/step\n",
            "Epoch 48/150\n",
            "11/11 - 0s - loss: 0.0991 - accuracy: 0.9689 - val_loss: 1.4181 - val_accuracy: 0.5487 - 110ms/epoch - 10ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "LSTM classifier trained, with validation accuracy 0.8851817042606516.\n",
            "Confusion matrix: \n",
            "          Pred:pos  Pred:neg\n",
            "True:pos        51         5\n",
            "True:neg         8        49.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shallow_cnn = False\n",
        "# ### 1dCNN classifier\n",
        "if shallow_cnn == True:\n",
        "    print(f\"Check shallow_cnn argument={shallow_cnn}, use the shallow structure.\")\n",
        "    classifier = Classifier(n_timesteps_padded, n_features) # shallow CNN for small data size\n",
        "else:\n",
        "    classifier = Classifier(n_timesteps_padded, n_features, n_conv_layers=3, add_dense_layer=False) # deeper CNN layers for data with larger size\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "classifier_history = classifier.fit(X_train_processed_padded,\n",
        "        y_train_classes,\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        verbose=True,\n",
        "        validation_data=(X_test_processed_padded, y_test_classes),\n",
        "        callbacks=[early_stopping_accuracy])\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "# y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"1dCNN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "        confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "        index=['True:pos', 'True:neg'],\n",
        "        columns=['Pred:pos', 'Pred:neg']\n",
        "    )\n",
        "confusion_matrix_df"
      ],
      "metadata": {
        "id": "GBp7UTpPL7cY",
        "outputId": "c12a6914-7944-4d5b-f8f9-345df1eb1e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "15/15 [==============================] - 3s 35ms/step - loss: 0.5416 - accuracy: 0.7622 - val_loss: 0.6624 - val_accuracy: 0.5133\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4217 - accuracy: 0.8133 - val_loss: 0.6622 - val_accuracy: 0.5221\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3188 - accuracy: 0.8711 - val_loss: 0.6371 - val_accuracy: 0.5487\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3593 - accuracy: 0.8378 - val_loss: 0.6095 - val_accuracy: 0.5929\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3774 - accuracy: 0.8511 - val_loss: 1.6900 - val_accuracy: 0.5133\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3351 - accuracy: 0.8489 - val_loss: 0.5945 - val_accuracy: 0.6372\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2872 - accuracy: 0.8822 - val_loss: 0.5388 - val_accuracy: 0.7168\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2672 - accuracy: 0.8956 - val_loss: 0.6555 - val_accuracy: 0.5044\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3310 - accuracy: 0.8467 - val_loss: 0.5477 - val_accuracy: 0.7080\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2347 - accuracy: 0.8978 - val_loss: 0.6033 - val_accuracy: 0.6726\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.2371 - accuracy: 0.9000 - val_loss: 0.4499 - val_accuracy: 0.8761\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3183 - accuracy: 0.8711 - val_loss: 0.6931 - val_accuracy: 0.5310\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2930 - accuracy: 0.8689 - val_loss: 0.7663 - val_accuracy: 0.6106\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2430 - accuracy: 0.8933 - val_loss: 0.8946 - val_accuracy: 0.5929\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2005 - accuracy: 0.9067 - val_loss: 1.0472 - val_accuracy: 0.5752\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2289 - accuracy: 0.9022 - val_loss: 0.8229 - val_accuracy: 0.6460\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2625 - accuracy: 0.8956 - val_loss: 0.9855 - val_accuracy: 0.5929\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1927 - accuracy: 0.9244 - val_loss: 0.4273 - val_accuracy: 0.7876\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2252 - accuracy: 0.9022 - val_loss: 1.0305 - val_accuracy: 0.5133\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1593 - accuracy: 0.9267 - val_loss: 0.6539 - val_accuracy: 0.6814\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.2342 - accuracy: 0.8733 - val_loss: 0.8432 - val_accuracy: 0.6726\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2263 - accuracy: 0.8889 - val_loss: 0.4037 - val_accuracy: 0.8496\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.1983 - accuracy: 0.9089 - val_loss: 0.6815 - val_accuracy: 0.6726\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1814 - accuracy: 0.9267 - val_loss: 0.4089 - val_accuracy: 0.8584\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1871 - accuracy: 0.9267 - val_loss: 0.5012 - val_accuracy: 0.7522\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2394 - accuracy: 0.8978 - val_loss: 0.6128 - val_accuracy: 0.6814\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2227 - accuracy: 0.9044 - val_loss: 2.4874 - val_accuracy: 0.5929\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1941 - accuracy: 0.9200 - val_loss: 1.2184 - val_accuracy: 0.7168\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.2966 - accuracy: 0.8756 - val_loss: 0.9121 - val_accuracy: 0.7080\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.2151 - accuracy: 0.9133 - val_loss: 0.7248 - val_accuracy: 0.7434\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1764 - accuracy: 0.9311 - val_loss: 0.5523 - val_accuracy: 0.7080\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.1391 - accuracy: 0.9467 - val_loss: 0.4834 - val_accuracy: 0.7699\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.1285 - accuracy: 0.9511 - val_loss: 0.4815 - val_accuracy: 0.8496\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.1243 - accuracy: 0.9511 - val_loss: 0.5729 - val_accuracy: 0.8053\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1228 - accuracy: 0.9533 - val_loss: 0.3973 - val_accuracy: 0.8230\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1449 - accuracy: 0.9311 - val_loss: 0.6589 - val_accuracy: 0.7611\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1320 - accuracy: 0.9511 - val_loss: 0.5751 - val_accuracy: 0.6991\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1298 - accuracy: 0.9533 - val_loss: 1.7267 - val_accuracy: 0.5398\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2420 - accuracy: 0.8867 - val_loss: 0.8937 - val_accuracy: 0.6372\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1293 - accuracy: 0.9511 - val_loss: 1.3544 - val_accuracy: 0.7257\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1153 - accuracy: 0.9578 - val_loss: 0.6399 - val_accuracy: 0.8319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Autoencoder(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    def Encoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        return x\n",
        "\n",
        "    def Decoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        # x = keras.layers.Conv1D(filters=64, kernel_size=2, activation=\"relu\")(x)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=1, kernel_size=3, activation=\"linear\", padding=\"same\"\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input = keras.Input(shape=(n_timesteps,n_features))\n",
        "    autoencoder = keras.Model(inputs=orig_input, outputs=Decoder(Encoder(orig_input)))\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def AutoencoderLSTM(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    # structure from medium post: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
        "    def EncoderLSTM(input):\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(input)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(input)\n",
        "        # encoded = keras.layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "        encoded = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(x)\n",
        "        return encoded\n",
        "\n",
        "    def DecoderLSTM(encoded):\n",
        "        x = keras.layers.RepeatVector(n_timesteps)(encoded)\n",
        "        # x = keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True)(x)\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x)\n",
        "        decoded = keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(n_features, activation=\"sigmoid\")\n",
        "        )(x)\n",
        "        return decoded\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input2 = keras.Input(shape=( n_timesteps,n_features))\n",
        "\n",
        "    autoencoder2 = keras.Model(\n",
        "        inputs=orig_input2, outputs=DecoderLSTM(EncoderLSTM(orig_input2))\n",
        "    )\n",
        "\n",
        "    return autoencoder2\n"
      ],
      "metadata": {
        "id": "YopwiyfxDr_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded.shape"
      ],
      "metadata": {
        "id": "0kU7VTl9Ev-I",
        "outputId": "37266952-8334-4d39-c8a4-fbb3dc79d349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(450, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_processed_padded.shape"
      ],
      "metadata": {
        "id": "2LaZyeYcH6a4",
        "outputId": "62ec829e-f897-4e48-c8d5-ffff0556061d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(113, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded = np.transpose(X_train_processed_padded, (0, 2, 1))"
      ],
      "metadata": {
        "id": "AL7pGcHjIKvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_processed_padded = np.transpose(X_test_processed_padded, (0, 2, 1))"
      ],
      "metadata": {
        "id": "zDaWXppWITcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "a6cbf655-b4d5-49a8-a3c0-f61a36f085fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 7s - loss: 0.0643 - val_loss: 0.0138 - 7s/epoch - 471ms/step\n",
            "Epoch 2/50\n",
            "15/15 - 3s - loss: 0.0045 - val_loss: 0.0013 - 3s/epoch - 192ms/step\n",
            "Epoch 3/50\n",
            "15/15 - 3s - loss: 0.0014 - val_loss: 0.0012 - 3s/epoch - 173ms/step\n",
            "Epoch 4/50\n",
            "15/15 - 2s - loss: 0.0011 - val_loss: 9.0219e-04 - 2s/epoch - 110ms/step\n",
            "Epoch 5/50\n",
            "15/15 - 2s - loss: 8.1441e-04 - val_loss: 6.6226e-04 - 2s/epoch - 109ms/step\n",
            "Epoch 6/50\n",
            "15/15 - 2s - loss: 6.2639e-04 - val_loss: 6.2277e-04 - 2s/epoch - 108ms/step\n",
            "Epoch 7/50\n",
            "15/15 - 2s - loss: 5.9381e-04 - val_loss: 5.9856e-04 - 2s/epoch - 107ms/step\n",
            "Epoch 8/50\n",
            "15/15 - 2s - loss: 5.7754e-04 - val_loss: 5.8973e-04 - 2s/epoch - 107ms/step\n",
            "Epoch 9/50\n",
            "15/15 - 2s - loss: 5.7019e-04 - val_loss: 5.8374e-04 - 2s/epoch - 118ms/step\n",
            "Epoch 10/50\n",
            "15/15 - 3s - loss: 5.6577e-04 - val_loss: 5.8037e-04 - 3s/epoch - 203ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.0005803668755106628.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoderLSTM = AutoencoderLSTM(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoderLSTM.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoderLSTM.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ],
      "metadata": {
        "id": "C5Sk6xWIKxMd",
        "outputId": "c08026f2-afd6-4c05-c721-e7d3bd595e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 45s - loss: 0.0183 - val_loss: 0.0093 - 45s/epoch - 3s/step\n",
            "Epoch 2/50\n",
            "15/15 - 33s - loss: 0.0106 - val_loss: 0.0085 - 33s/epoch - 2s/step\n",
            "Epoch 3/50\n",
            "15/15 - 33s - loss: 0.0101 - val_loss: 0.0077 - 33s/epoch - 2s/step\n",
            "Epoch 4/50\n",
            "15/15 - 33s - loss: 0.0080 - val_loss: 0.0048 - 33s/epoch - 2s/step\n",
            "Epoch 5/50\n",
            "15/15 - 33s - loss: 0.0089 - val_loss: 0.0082 - 33s/epoch - 2s/step\n",
            "Epoch 6/50\n",
            "15/15 - 33s - loss: 0.0069 - val_loss: 0.0047 - 33s/epoch - 2s/step\n",
            "Epoch 7/50\n",
            "15/15 - 33s - loss: 0.0051 - val_loss: 0.0041 - 33s/epoch - 2s/step\n",
            "Epoch 8/50\n",
            "15/15 - 34s - loss: 0.0046 - val_loss: 0.0038 - 34s/epoch - 2s/step\n",
            "Epoch 9/50\n",
            "15/15 - 33s - loss: 0.0045 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 10/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0033 - 33s/epoch - 2s/step\n",
            "Epoch 11/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0060 - 33s/epoch - 2s/step\n",
            "Epoch 12/50\n",
            "15/15 - 34s - loss: 0.0055 - val_loss: 0.0042 - 34s/epoch - 2s/step\n",
            "Epoch 13/50\n",
            "15/15 - 34s - loss: 0.0045 - val_loss: 0.0038 - 34s/epoch - 2s/step\n",
            "Epoch 14/50\n",
            "15/15 - 33s - loss: 0.0040 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 15/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0033 - 33s/epoch - 2s/step\n",
            "Epoch 16/50\n",
            "15/15 - 34s - loss: 0.0038 - val_loss: 0.0035 - 34s/epoch - 2s/step\n",
            "Epoch 17/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 18/50\n",
            "15/15 - 35s - loss: 0.0035 - val_loss: 0.0033 - 35s/epoch - 2s/step\n",
            "Epoch 19/50\n",
            "15/15 - 33s - loss: 0.0038 - val_loss: 0.0042 - 33s/epoch - 2s/step\n",
            "Epoch 20/50\n",
            "15/15 - 33s - loss: 0.0053 - val_loss: 0.0039 - 33s/epoch - 2s/step\n",
            "Epoch 21/50\n",
            "15/15 - 33s - loss: 0.0043 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 22/50\n",
            "15/15 - 33s - loss: 0.0042 - val_loss: 0.0035 - 33s/epoch - 2s/step\n",
            "Epoch 23/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0036 - 33s/epoch - 2s/step\n",
            "Epoch 24/50\n",
            "15/15 - 33s - loss: 0.0045 - val_loss: 0.0044 - 33s/epoch - 2s/step\n",
            "Epoch 25/50\n",
            "15/15 - 33s - loss: 0.0040 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 26/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 27/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 28/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 29/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 30/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 31/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 32/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 33/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0063 - 33s/epoch - 2s/step\n",
            "Epoch 34/50\n",
            "15/15 - 33s - loss: 0.0055 - val_loss: 0.0038 - 33s/epoch - 2s/step\n",
            "Epoch 35/50\n",
            "15/15 - 33s - loss: 0.0047 - val_loss: 0.0039 - 33s/epoch - 2s/step\n",
            "Epoch 36/50\n",
            "15/15 - 33s - loss: 0.0044 - val_loss: 0.0036 - 33s/epoch - 2s/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.0030726369004696608.\n"
          ]
        }
      ]
    }
  ]
}