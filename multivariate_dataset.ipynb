{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/learning-time-series-counterfactuals/blob/main/multivariate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIiG4khZBur-",
        "outputId": "2618e3db-7b5d-417a-ea00-250a3af75005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'learning-time-series-counterfactuals' already exists and is not an empty directory.\n",
            "/content/learning-time-series-counterfactuals\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/stellagerantoni/learning-time-series-counterfactuals\n",
        "%cd learning-time-series-counterfactuals/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon[all_extras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89L3kts7CCan",
        "outputId": "341ed035-5b23-4d45-8132-d081c4b6823e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aeon[all_extras] in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1.0)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.14)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.56.4)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.3.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.2.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: filterpy>=1.4.5 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.9.0)\n",
            "Requirement already satisfied: hmmlearn>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: gluonts>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.13.4)\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.51.0)\n",
            "Requirement already satisfied: kotsu>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.3.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.7.1)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.1)\n",
            "Requirement already satisfied: pmdarima<3.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.0.3)\n",
            "Requirement already satisfied: prophet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.4)\n",
            "Requirement already satisfied: pyod>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: scikit-posthocs>=0.6.5 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.7.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.12.2)\n",
            "Requirement already satisfied: statsforecast>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.6.0)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.14.0)\n",
            "Requirement already satisfied: stumpy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.12.0)\n",
            "Requirement already satisfied: tbats>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.3)\n",
            "Requirement already satisfied: tensorflow<2.13.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.12.1)\n",
            "Requirement already satisfied: tensorflow-probability<0.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.20.1)\n",
            "Requirement already satisfied: tsfresh>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.20.1)\n",
            "Requirement already satisfied: tslearn<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.5.3.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.7.0)\n",
            "Requirement already satisfied: mlflow<2.4.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.3.2)\n",
            "Requirement already satisfied: esig<0.9.8.3,>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.9.8.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon[all_extras]) (1.14.1)\n",
            "Requirement already satisfied: pydantic~=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (1.10.12)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.66.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (8.1.7)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.17.7)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.35)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.31.0)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4.4)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (1.12.0)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.1.3)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.2.5)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (1.2.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.0.20)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.4.4)\n",
            "Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (20.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (0.29.36)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (1.26.16)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.32)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.8.0->aeon[all_extras]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3.0,>=1.0.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (from statsforecast>=0.5.2->aeon[all_extras]) (0.17.3)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=0.5.2->aeon[all_extras]) (0.8.6)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.1->aeon[all_extras]) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.57.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.33.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (0.1.8)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.20.0->aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (1.7.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow<2.4.0->aeon[all_extras]) (1.2.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13.0->aeon[all_extras]) (0.41.2)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->prophet>=1.1.0->aeon[all_extras]) (0.5.12)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (0.9.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (6.3.2)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow<2.4.0->aeon[all_extras]) (1.6.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.3.7)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.1.2)\n",
            "Requirement already satisfied: triad>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.9.1)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.1.6)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (18.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras]) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow<2.4.0->aeon[all_extras]) (3.16.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow<2.4.0->aeon[all_extras]) (2.1.3)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet>=1.1.0->aeon[all_extras]) (4.1.4)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->aeon[all_extras]) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.4.0->aeon[all_extras]) (2.0.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.7.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12,>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (4.11.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras]) (5.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.3.1)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (2.4.16)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.5.0)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "\n",
        "from _composite import ModifiedLatentCF\n",
        "%cd src\n",
        "from _vanilla import LatentCF\n",
        "from help_functions import (ResultWriter, conditional_pad, evaluate,\n",
        "                            find_best_lr, plot_graphs,\n",
        "                            reset_seeds, time_series_normalize,\n",
        "                            time_series_revert, upsample_minority,\n",
        "                            validity_score)\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "aaeb67f5-70a9-4b84-8952-4c42dfdd3ab7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/learning-time-series-counterfactuals/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FUNCTIONS**"
      ],
      "metadata": {
        "id": "IHBw9E_4Zm5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y, meta_data = load_classification(dataset)\n",
        "  if dataset == 'Heartbeat':\n",
        "    pos = 'normal'\n",
        "    neg = 'abnormal'\n",
        "    X = X.transpose(0,2,1)\n",
        "  if dataset == 'SelfRegulationSCP1':\n",
        "    pos = 'positivity'\n",
        "    neg = 'negativity'\n",
        "    X = X.transpose(0,2,1)\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y,pos_label, neg_label"
      ],
      "metadata": {
        "id": "QsJJktx22dXs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normalize_multivariate(data, n_timesteps, n_features, scaler=None):\n",
        "\n",
        "    # Then reshape data to have timesteps as rows for normalization\n",
        "    data_reshaped = data.reshape(-1, n_features)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaler.fit(data_reshaped)\n",
        "\n",
        "    normalized = scaler.transform(data_reshaped)\n",
        "\n",
        "    # Return data reshaped\n",
        "    data = normalized.reshape(-1, n_timesteps, n_features)\n",
        "    return data, scaler\n",
        "\n",
        "def conditional_pad_multivariate(X):\n",
        "    num_timesteps = X.shape[1]\n",
        "\n",
        "    if num_timesteps % 4 != 0:\n",
        "        next_num = (int(num_timesteps / 4) + 1) * 4\n",
        "        padding_size = next_num - num_timesteps\n",
        "        X_padded = np.pad(\n",
        "            X, pad_width=((0, 0), (0, padding_size), (0, 0))\n",
        "        )\n",
        "\n",
        "        return X_padded, padding_size\n",
        "\n",
        "    return X, 0\n",
        "\n"
      ],
      "metadata": {
        "id": "MQmQpShMijLz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**\n",
        "datasets available : 'Heartbeat', 'SelfRegulationSCP1'"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,pos_label, neg_label = load_dataset('SelfRegulationSCP1')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "00f9decb-3b28-4b7b-ca8c-601489355d81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (561, 896, 6)\n",
            " Shape of y =  (561,)\n",
            " Meta data =  {'problemname': 'selfregulationscp1', 'timestamps': False, 'missing': False, 'univariate': False, 'equallength': True, 'classlabel': True, 'targetlabel': False, 'class_values': ['negativity', 'positivity']}\n",
            "\n",
            " X[:1] = \n",
            "[[[23.   19.03 32.19 43.66 30.72 39.09]\n",
            "  [21.66 19.19 36.81 41.22 31.81 39.53]\n",
            "  [20.84 21.5  40.16 39.69 31.69 40.59]\n",
            "  ...\n",
            "  [21.84 27.03 32.5  50.06 44.69 44.03]\n",
            "  [22.62 26.78 34.53 50.84 43.69 44.75]\n",
            "  [21.5  28.94 37.12 50.75 41.88 46.19]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsample the minority class\n",
        "\n",
        "pos_counts = pd.value_counts(y_train)[pos_label]\n",
        "neg_counts = pd.value_counts(y_train)[neg_label]\n",
        "print(f\"negative_count = {neg_counts}, positive_count = {pos_counts}\")\n",
        "\n",
        "if pos_counts!=neg_counts:\n",
        "  X_train, y_train = upsample_minority(X_train, y_train, pos_label=pos_label, neg_label=neg_label)\n",
        "  print(f\"Data upsampling performed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n",
        "else:\n",
        "   print(f\"Data upsampling not needed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "0a8e90f9-6313-4c39-e1d9-83170422a433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative_count = 225, positive_count = 223\n",
            "Data upsampling performed, current distribution of y: \n",
            "0    225\n",
            "1    225\n",
            "dtype: int64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n",
        "\n",
        "y_train_classes = y_train\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "be2bf90d-b724-423c-bac1-784633ce0e30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=896, padded #timesteps=896.\n",
            "\n",
            "min value = -73.84, max value = 175.06\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (450, 896, 6)\n",
            "X_train_processed_padded.shape = (450, 896, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVeJo5c1cgGr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2\n",
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=1,input_format_stf=True\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    if input_format_stf:\n",
        "      input_shape = (n_timesteps, n_features)\n",
        "    else:\n",
        "      input_shape = (n_features, n_timesteps)\n",
        "    inputs = keras.Input(shape=input_shape, dtype=\"float32\")\n",
        "\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "def LSTMFCNClassifier(n_timesteps, n_features, n_output=2, n_LSTM_cells=8, regularization_rate = 0.001,input_format_stf=True):\n",
        "    # https://github.com/titu1994/LSTM-FCN/blob/master/hyperparameter_search.py\n",
        "    if input_format_stf:\n",
        "      input_shape = (n_timesteps, n_features)\n",
        "    else:\n",
        "      input_shape = (n_features, n_timesteps)\n",
        "    inputs = keras.Input(shape=input_shape, dtype=\"float32\")\n",
        "\n",
        "    x = keras.layers.LSTM(units=n_LSTM_cells, kernel_regularizer=l2(regularization_rate))(inputs)\n",
        "    x = keras.layers.Dropout(rate=0.8)(x)\n",
        "\n",
        "    y = keras.layers.Permute((2, 1))(inputs)\n",
        "    y = keras.layers.Conv1D(64, 8, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(128, 5, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(64, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    x = keras.layers.concatenate([x, y])\n",
        "\n",
        "    outputs = keras.layers.Dense(n_output, activation=\"softmax\", kernel_regularizer=l2(regularization_rate))(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def ClassifierLSTM(n_timesteps, n_features, extra_lstm_layer=True, n_output=1, input_format_stf = True):\n",
        "    # Define the model structure - only LSTM layers\n",
        "    # https://www.kaggle.com/szaitseff/classification-of-time-series-with-lstm-rnn\n",
        "\n",
        "    if input_format_stf:\n",
        "      input_shape = (n_timesteps, n_features)\n",
        "    else:\n",
        "      input_shape = (n_features, n_timesteps)\n",
        "    inputs = keras.Input(shape=input_shape, dtype=\"float32\")\n",
        "\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(\n",
        "            inputs\n",
        "        )  # set return_sequences true to feed next LSTM layer\n",
        "    else:\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(\n",
        "            inputs\n",
        "        )  # set return_sequences false to feed dense layer directly\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    # x = keras.layers.LSTM(32, activation='tanh', return_sequences=True)(x)\n",
        "    # x = keras.layers.BatchNormalization()(x)\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(16, activation=\"tanh\", return_sequences=False)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier2 = keras.Model(inputs, outputs)\n",
        "\n",
        "    return classifier2\n",
        "\n",
        "def Classifier_FCN(input_shape, nb_classes):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation=\"relu\")(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation(\"relu\")(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation(\"relu\")(conv3)\n",
        "\n",
        "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(nb_classes, activation=\"softmax\")(gap_layer)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def EnhancedClassifier(\n",
        "    n_timesteps,\n",
        "    n_features,\n",
        "    n_conv_layers=2,\n",
        "    add_dense_layer=True,\n",
        "    n_output=1,\n",
        "    input_format_stf=True # if True, input is (series, features, timesteps); otherwise (series, timesteps, features)\n",
        "):\n",
        "    # Choose input shape based on the flag\n",
        "    if input_format_stf:\n",
        "      input_shape = (n_features, n_timesteps)\n",
        "    else:\n",
        "      input_shape = (n_timesteps, n_features)\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape, dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.TimeDistributed(keras.layers.Dense(128))(x)\n",
        "\n",
        "    for _ in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\", activation='relu')(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "    x = keras.layers.Dense(64, activation=\"relu\")(x) # Additional dense layer for complexity\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "sD3qinpSEkWK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)"
      ],
      "metadata": {
        "id": "2tscqoRNiBo-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded.shape"
      ],
      "metadata": {
        "id": "IyhdJrc7iDDH",
        "outputId": "90a997e1-e76e-43d4-c6e5-c838b3138649",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(450, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_processed_padded.shape"
      ],
      "metadata": {
        "id": "Fp8er70PiEUs",
        "outputId": "8b7db853-8f66-4b7b-b069-5f6d1998fd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(113, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FORMAT_STF = True\n",
        "if INPUT_FORMAT_STF == False:\n",
        "  X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "  X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)\n",
        "\n",
        "n_lstmcells = 8\n",
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ## 2.0 LSTM-FCN classifier\n",
        "###############################################\n",
        "# ### LSTM-FCN classifier\n",
        "LSTMFCNclassifier = LSTMFCNClassifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, n_LSTM_cells=n_lstmcells,input_format_stf=INPUT_FORMAT_STF\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "LSTMFCNclassifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = LSTMFCNclassifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = LSTMFCNclassifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")\n",
        "\n",
        "if INPUT_FORMAT_STF == False:\n",
        "  X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "  X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "e82f9eba-288e-46db-eee1-e8cabbec907f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "15/15 [==============================] - 8s 85ms/step - loss: 1.0165 - accuracy: 0.7733 - val_loss: 2.2895 - val_accuracy: 0.5044\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.8766 - accuracy: 0.8467 - val_loss: 1.5305 - val_accuracy: 0.5310\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.8388 - accuracy: 0.8489 - val_loss: 1.4171 - val_accuracy: 0.5487\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.7627 - accuracy: 0.8578 - val_loss: 1.0453 - val_accuracy: 0.5841\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.7662 - accuracy: 0.8689 - val_loss: 1.9389 - val_accuracy: 0.5310\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.6856 - accuracy: 0.8822 - val_loss: 0.9491 - val_accuracy: 0.6991\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.6406 - accuracy: 0.8844 - val_loss: 0.7822 - val_accuracy: 0.8673\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.6183 - accuracy: 0.8867 - val_loss: 1.0414 - val_accuracy: 0.5133\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.5614 - accuracy: 0.9022 - val_loss: 1.3036 - val_accuracy: 0.4956\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.5657 - accuracy: 0.8778 - val_loss: 2.7273 - val_accuracy: 0.5398\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 0s 32ms/step - loss: 0.5819 - accuracy: 0.8889 - val_loss: 0.8232 - val_accuracy: 0.6637\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 1s 37ms/step - loss: 0.5738 - accuracy: 0.8822 - val_loss: 0.7687 - val_accuracy: 0.7876\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.5245 - accuracy: 0.9089 - val_loss: 1.0200 - val_accuracy: 0.5310\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.4761 - accuracy: 0.9267 - val_loss: 0.6952 - val_accuracy: 0.8053\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.4551 - accuracy: 0.9378 - val_loss: 1.0864 - val_accuracy: 0.6903\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.4629 - accuracy: 0.9133 - val_loss: 0.6951 - val_accuracy: 0.7788\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.4597 - accuracy: 0.9289 - val_loss: 0.7725 - val_accuracy: 0.7611\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.4805 - accuracy: 0.8956 - val_loss: 0.6176 - val_accuracy: 0.8584\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 1s 49ms/step - loss: 0.5060 - accuracy: 0.8978 - val_loss: 0.7385 - val_accuracy: 0.7345\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 1s 49ms/step - loss: 0.4528 - accuracy: 0.9222 - val_loss: 2.4536 - val_accuracy: 0.4956\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 1s 56ms/step - loss: 0.4634 - accuracy: 0.9000 - val_loss: 3.9697 - val_accuracy: 0.4956\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 0.4203 - accuracy: 0.9178 - val_loss: 4.3576 - val_accuracy: 0.4956\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.5315 - accuracy: 0.8578 - val_loss: 1.1987 - val_accuracy: 0.5221\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 1s 56ms/step - loss: 0.4924 - accuracy: 0.8644 - val_loss: 0.6130 - val_accuracy: 0.8319\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 1s 87ms/step - loss: 0.4697 - accuracy: 0.9022 - val_loss: 0.5743 - val_accuracy: 0.8142\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.4623 - accuracy: 0.8978 - val_loss: 1.1244 - val_accuracy: 0.7257\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 1s 72ms/step - loss: 0.4133 - accuracy: 0.9200 - val_loss: 0.6779 - val_accuracy: 0.7257\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.4051 - accuracy: 0.9289 - val_loss: 0.7332 - val_accuracy: 0.6814\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3992 - accuracy: 0.9289 - val_loss: 2.7154 - val_accuracy: 0.4956\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.4006 - accuracy: 0.9067 - val_loss: 1.1105 - val_accuracy: 0.5310\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.3872 - accuracy: 0.9111 - val_loss: 0.5269 - val_accuracy: 0.8761\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.3453 - accuracy: 0.9422 - val_loss: 0.7159 - val_accuracy: 0.8053\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.4050 - accuracy: 0.9044 - val_loss: 2.4900 - val_accuracy: 0.5664\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3885 - accuracy: 0.9178 - val_loss: 0.8944 - val_accuracy: 0.6903\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 0s 34ms/step - loss: 0.4023 - accuracy: 0.9111 - val_loss: 0.8451 - val_accuracy: 0.7434\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.3328 - accuracy: 0.9444 - val_loss: 0.5001 - val_accuracy: 0.8761\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.3743 - accuracy: 0.9178 - val_loss: 0.7078 - val_accuracy: 0.7611\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3753 - accuracy: 0.9178 - val_loss: 0.5918 - val_accuracy: 0.7611\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.3519 - accuracy: 0.9289 - val_loss: 2.5443 - val_accuracy: 0.4956\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 1s 36ms/step - loss: 0.4120 - accuracy: 0.8889 - val_loss: 1.5155 - val_accuracy: 0.4956\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3667 - accuracy: 0.9178 - val_loss: 0.8690 - val_accuracy: 0.6106\n",
            "Epoch 42/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3532 - accuracy: 0.9222 - val_loss: 2.8830 - val_accuracy: 0.4956\n",
            "Epoch 43/150\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.3856 - accuracy: 0.9111 - val_loss: 2.1085 - val_accuracy: 0.4956\n",
            "Epoch 44/150\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.3733 - accuracy: 0.9156 - val_loss: 2.2415 - val_accuracy: 0.4956\n",
            "Epoch 45/150\n",
            "15/15 [==============================] - 1s 55ms/step - loss: 0.3230 - accuracy: 0.9356 - val_loss: 1.0136 - val_accuracy: 0.4956\n",
            "Epoch 46/150\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 0.3137 - accuracy: 0.9222 - val_loss: 1.1316 - val_accuracy: 0.4956\n",
            "Epoch 47/150\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.3081 - accuracy: 0.9400 - val_loss: 1.1318 - val_accuracy: 0.5487\n",
            "Epoch 48/150\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.2794 - accuracy: 0.9356 - val_loss: 2.6125 - val_accuracy: 0.4956\n",
            "Epoch 49/150\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.4168 - accuracy: 0.8978 - val_loss: 1.0077 - val_accuracy: 0.5487\n",
            "Epoch 50/150\n",
            "15/15 [==============================] - 1s 54ms/step - loss: 0.3538 - accuracy: 0.9244 - val_loss: 0.6834 - val_accuracy: 0.7345\n",
            "Epoch 51/150\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.3181 - accuracy: 0.9311 - val_loss: 2.5527 - val_accuracy: 0.4956\n",
            "Epoch 52/150\n",
            "15/15 [==============================] - 1s 51ms/step - loss: 0.3957 - accuracy: 0.8867 - val_loss: 4.3707 - val_accuracy: 0.5221\n",
            "Epoch 53/150\n",
            "15/15 [==============================] - 1s 52ms/step - loss: 0.3172 - accuracy: 0.9444 - val_loss: 2.4201 - val_accuracy: 0.5664\n",
            "Epoch 54/150\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.3619 - accuracy: 0.9178 - val_loss: 1.4686 - val_accuracy: 0.5929\n",
            "Epoch 55/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.3045 - accuracy: 0.9467 - val_loss: 0.6535 - val_accuracy: 0.8053\n",
            "Epoch 56/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.2970 - accuracy: 0.9378 - val_loss: 0.5306 - val_accuracy: 0.8230\n",
            "Epoch 57/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.2809 - accuracy: 0.9422 - val_loss: 3.0634 - val_accuracy: 0.4956\n",
            "Epoch 58/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.5152 - accuracy: 0.8222 - val_loss: 5.1283 - val_accuracy: 0.5221\n",
            "Epoch 59/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.4246 - accuracy: 0.8733 - val_loss: 2.1757 - val_accuracy: 0.5664\n",
            "Epoch 60/150\n",
            "15/15 [==============================] - 1s 34ms/step - loss: 0.3812 - accuracy: 0.9178 - val_loss: 0.5361 - val_accuracy: 0.8142\n",
            "Epoch 61/150\n",
            "15/15 [==============================] - 1s 35ms/step - loss: 0.3480 - accuracy: 0.9356 - val_loss: 0.5196 - val_accuracy: 0.8319\n",
            "4/4 [==============================] - 1s 13ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.87578320802005.\n",
            "Confusion matrix: \n",
            "          Pred:pos  Pred:neg\n",
            "True:pos        47         9\n",
            "True:neg         5        52.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "INPUT_FORMAT_STF = False\n",
        "\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### LSTM classifier\n",
        "if INPUT_FORMAT_STF == False:\n",
        "    X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "    X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)\n",
        "\n",
        "classifierLSTM = ClassifierLSTM(n_timesteps_padded, n_features,extra_lstm_layer=True, n_output=2, input_format_stf=INPUT_FORMAT_STF)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "classifierLSTM.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM classifier:\")\n",
        "classifier_history = classifierLSTM.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=42,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifierLSTM.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")\n",
        "if INPUT_FORMAT_STF == False:\n",
        "  X_train_processed_padded = X_train_processed_padded.transpose(0,2,1)\n",
        "  X_test_processed_padded = X_test_processed_padded.transpose(0,2,1)"
      ],
      "metadata": {
        "id": "fvEe9-iRwX-h",
        "outputId": "e28291d0-e9bb-40cf-b13f-0f4f584d89e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM classifier:\n",
            "Epoch 1/150\n",
            "11/11 - 6s - loss: 0.4475 - accuracy: 0.8244 - val_loss: 0.6779 - val_accuracy: 0.5044 - 6s/epoch - 526ms/step\n",
            "Epoch 2/150\n",
            "11/11 - 0s - loss: 0.3421 - accuracy: 0.8556 - val_loss: 0.6656 - val_accuracy: 0.5841 - 122ms/epoch - 11ms/step\n",
            "Epoch 3/150\n",
            "11/11 - 0s - loss: 0.2951 - accuracy: 0.8756 - val_loss: 0.6597 - val_accuracy: 0.7434 - 121ms/epoch - 11ms/step\n",
            "Epoch 4/150\n",
            "11/11 - 0s - loss: 0.2900 - accuracy: 0.8733 - val_loss: 0.6604 - val_accuracy: 0.6814 - 122ms/epoch - 11ms/step\n",
            "Epoch 5/150\n",
            "11/11 - 0s - loss: 0.2743 - accuracy: 0.8956 - val_loss: 0.6405 - val_accuracy: 0.7876 - 132ms/epoch - 12ms/step\n",
            "Epoch 6/150\n",
            "11/11 - 0s - loss: 0.2483 - accuracy: 0.8978 - val_loss: 0.6588 - val_accuracy: 0.5133 - 125ms/epoch - 11ms/step\n",
            "Epoch 7/150\n",
            "11/11 - 0s - loss: 0.2318 - accuracy: 0.9089 - val_loss: 0.6475 - val_accuracy: 0.5487 - 142ms/epoch - 13ms/step\n",
            "Epoch 8/150\n",
            "11/11 - 0s - loss: 0.2248 - accuracy: 0.9133 - val_loss: 0.6892 - val_accuracy: 0.4956 - 127ms/epoch - 12ms/step\n",
            "Epoch 9/150\n",
            "11/11 - 0s - loss: 0.2078 - accuracy: 0.9133 - val_loss: 0.7675 - val_accuracy: 0.4956 - 120ms/epoch - 11ms/step\n",
            "Epoch 10/150\n",
            "11/11 - 0s - loss: 0.2474 - accuracy: 0.9022 - val_loss: 0.6113 - val_accuracy: 0.8053 - 133ms/epoch - 12ms/step\n",
            "Epoch 11/150\n",
            "11/11 - 0s - loss: 0.2019 - accuracy: 0.9156 - val_loss: 0.6167 - val_accuracy: 0.7611 - 165ms/epoch - 15ms/step\n",
            "Epoch 12/150\n",
            "11/11 - 0s - loss: 0.2014 - accuracy: 0.9400 - val_loss: 0.5834 - val_accuracy: 0.8230 - 181ms/epoch - 16ms/step\n",
            "Epoch 13/150\n",
            "11/11 - 0s - loss: 0.1921 - accuracy: 0.9244 - val_loss: 0.5671 - val_accuracy: 0.7434 - 155ms/epoch - 14ms/step\n",
            "Epoch 14/150\n",
            "11/11 - 0s - loss: 0.1705 - accuracy: 0.9333 - val_loss: 0.5848 - val_accuracy: 0.7257 - 216ms/epoch - 20ms/step\n",
            "Epoch 15/150\n",
            "11/11 - 0s - loss: 0.1647 - accuracy: 0.9400 - val_loss: 0.5391 - val_accuracy: 0.8673 - 208ms/epoch - 19ms/step\n",
            "Epoch 16/150\n",
            "11/11 - 0s - loss: 0.1938 - accuracy: 0.9133 - val_loss: 0.5575 - val_accuracy: 0.6283 - 226ms/epoch - 21ms/step\n",
            "Epoch 17/150\n",
            "11/11 - 0s - loss: 0.1786 - accuracy: 0.9400 - val_loss: 0.5198 - val_accuracy: 0.7788 - 225ms/epoch - 20ms/step\n",
            "Epoch 18/150\n",
            "11/11 - 0s - loss: 0.1541 - accuracy: 0.9533 - val_loss: 0.4890 - val_accuracy: 0.7611 - 266ms/epoch - 24ms/step\n",
            "Epoch 19/150\n",
            "11/11 - 0s - loss: 0.1677 - accuracy: 0.9289 - val_loss: 0.4339 - val_accuracy: 0.8850 - 245ms/epoch - 22ms/step\n",
            "Epoch 20/150\n",
            "11/11 - 0s - loss: 0.1509 - accuracy: 0.9444 - val_loss: 0.4183 - val_accuracy: 0.8761 - 222ms/epoch - 20ms/step\n",
            "Epoch 21/150\n",
            "11/11 - 0s - loss: 0.1347 - accuracy: 0.9600 - val_loss: 0.4716 - val_accuracy: 0.7876 - 230ms/epoch - 21ms/step\n",
            "Epoch 22/150\n",
            "11/11 - 0s - loss: 0.1449 - accuracy: 0.9400 - val_loss: 0.5073 - val_accuracy: 0.7345 - 222ms/epoch - 20ms/step\n",
            "Epoch 23/150\n",
            "11/11 - 0s - loss: 0.1420 - accuracy: 0.9533 - val_loss: 0.5751 - val_accuracy: 0.6549 - 245ms/epoch - 22ms/step\n",
            "Epoch 24/150\n",
            "11/11 - 0s - loss: 0.1228 - accuracy: 0.9533 - val_loss: 0.3658 - val_accuracy: 0.8761 - 239ms/epoch - 22ms/step\n",
            "Epoch 25/150\n",
            "11/11 - 0s - loss: 0.1457 - accuracy: 0.9467 - val_loss: 0.3991 - val_accuracy: 0.8673 - 255ms/epoch - 23ms/step\n",
            "Epoch 26/150\n",
            "11/11 - 0s - loss: 0.1129 - accuracy: 0.9711 - val_loss: 0.4869 - val_accuracy: 0.8230 - 217ms/epoch - 20ms/step\n",
            "Epoch 27/150\n",
            "11/11 - 0s - loss: 0.1053 - accuracy: 0.9667 - val_loss: 0.4096 - val_accuracy: 0.8319 - 217ms/epoch - 20ms/step\n",
            "Epoch 28/150\n",
            "11/11 - 0s - loss: 0.0905 - accuracy: 0.9689 - val_loss: 0.5449 - val_accuracy: 0.7345 - 209ms/epoch - 19ms/step\n",
            "Epoch 29/150\n",
            "11/11 - 0s - loss: 0.0996 - accuracy: 0.9644 - val_loss: 0.8803 - val_accuracy: 0.5487 - 238ms/epoch - 22ms/step\n",
            "Epoch 30/150\n",
            "11/11 - 0s - loss: 0.1102 - accuracy: 0.9533 - val_loss: 0.4988 - val_accuracy: 0.7965 - 207ms/epoch - 19ms/step\n",
            "Epoch 31/150\n",
            "11/11 - 0s - loss: 0.1302 - accuracy: 0.9600 - val_loss: 0.4319 - val_accuracy: 0.8496 - 191ms/epoch - 17ms/step\n",
            "Epoch 32/150\n",
            "11/11 - 0s - loss: 0.1737 - accuracy: 0.9378 - val_loss: 0.7538 - val_accuracy: 0.5398 - 231ms/epoch - 21ms/step\n",
            "Epoch 33/150\n",
            "11/11 - 0s - loss: 0.1423 - accuracy: 0.9378 - val_loss: 1.9065 - val_accuracy: 0.4956 - 180ms/epoch - 16ms/step\n",
            "Epoch 34/150\n",
            "11/11 - 0s - loss: 0.1119 - accuracy: 0.9644 - val_loss: 1.6740 - val_accuracy: 0.4956 - 208ms/epoch - 19ms/step\n",
            "Epoch 35/150\n",
            "11/11 - 0s - loss: 0.0965 - accuracy: 0.9756 - val_loss: 1.4020 - val_accuracy: 0.5133 - 212ms/epoch - 19ms/step\n",
            "Epoch 36/150\n",
            "11/11 - 0s - loss: 0.0743 - accuracy: 0.9867 - val_loss: 1.9225 - val_accuracy: 0.4956 - 206ms/epoch - 19ms/step\n",
            "Epoch 37/150\n",
            "11/11 - 0s - loss: 0.0911 - accuracy: 0.9711 - val_loss: 1.2937 - val_accuracy: 0.5310 - 227ms/epoch - 21ms/step\n",
            "Epoch 38/150\n",
            "11/11 - 0s - loss: 0.0824 - accuracy: 0.9778 - val_loss: 1.3895 - val_accuracy: 0.5310 - 202ms/epoch - 18ms/step\n",
            "Epoch 39/150\n",
            "11/11 - 0s - loss: 0.0976 - accuracy: 0.9600 - val_loss: 1.3654 - val_accuracy: 0.5044 - 251ms/epoch - 23ms/step\n",
            "Epoch 40/150\n",
            "11/11 - 0s - loss: 0.0998 - accuracy: 0.9644 - val_loss: 1.9910 - val_accuracy: 0.5044 - 204ms/epoch - 19ms/step\n",
            "Epoch 41/150\n",
            "11/11 - 0s - loss: 0.0716 - accuracy: 0.9756 - val_loss: 1.5492 - val_accuracy: 0.5310 - 283ms/epoch - 26ms/step\n",
            "Epoch 42/150\n",
            "11/11 - 0s - loss: 0.1118 - accuracy: 0.9644 - val_loss: 1.5058 - val_accuracy: 0.4956 - 206ms/epoch - 19ms/step\n",
            "Epoch 43/150\n",
            "11/11 - 0s - loss: 0.0685 - accuracy: 0.9822 - val_loss: 0.7951 - val_accuracy: 0.5841 - 377ms/epoch - 34ms/step\n",
            "Epoch 44/150\n",
            "11/11 - 0s - loss: 0.0723 - accuracy: 0.9800 - val_loss: 2.7984 - val_accuracy: 0.4956 - 422ms/epoch - 38ms/step\n",
            "Epoch 45/150\n",
            "11/11 - 0s - loss: 0.0596 - accuracy: 0.9844 - val_loss: 1.2444 - val_accuracy: 0.5929 - 261ms/epoch - 24ms/step\n",
            "Epoch 46/150\n",
            "11/11 - 0s - loss: 0.0415 - accuracy: 0.9956 - val_loss: 0.5686 - val_accuracy: 0.7522 - 226ms/epoch - 21ms/step\n",
            "Epoch 47/150\n",
            "11/11 - 0s - loss: 0.0541 - accuracy: 0.9844 - val_loss: 0.5568 - val_accuracy: 0.7168 - 394ms/epoch - 36ms/step\n",
            "Epoch 48/150\n",
            "11/11 - 0s - loss: 0.0362 - accuracy: 0.9933 - val_loss: 0.9459 - val_accuracy: 0.6106 - 313ms/epoch - 28ms/step\n",
            "Epoch 49/150\n",
            "11/11 - 0s - loss: 0.0390 - accuracy: 0.9889 - val_loss: 0.7127 - val_accuracy: 0.7788 - 427ms/epoch - 39ms/step\n",
            "4/4 [==============================] - 1s 5ms/step\n",
            "LSTM classifier trained, with validation accuracy 0.8842418546365916.\n",
            "Confusion matrix: \n",
            "          Pred:pos  Pred:neg\n",
            "True:pos        45        11\n",
            "True:neg         2        55.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Autoencoder(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    def Encoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        return x\n",
        "\n",
        "    def Decoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        # x = keras.layers.Conv1D(filters=64, kernel_size=2, activation=\"relu\")(x)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=1, kernel_size=3, activation=\"linear\", padding=\"same\"\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input = keras.Input(shape=(n_timesteps,n_features))\n",
        "    autoencoder = keras.Model(inputs=orig_input, outputs=Decoder(Encoder(orig_input)))\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def AutoencoderLSTM(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    # structure from medium post: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
        "    def EncoderLSTM(input):\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(input)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(input)\n",
        "        # encoded = keras.layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "        encoded = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(x)\n",
        "        return encoded\n",
        "\n",
        "    def DecoderLSTM(encoded):\n",
        "        x = keras.layers.RepeatVector(n_timesteps)(encoded)\n",
        "        # x = keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True)(x)\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x)\n",
        "        decoded = keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(n_features, activation=\"sigmoid\")\n",
        "        )(x)\n",
        "        return decoded\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input2 = keras.Input(shape=( n_timesteps,n_features))\n",
        "\n",
        "    autoencoder2 = keras.Model(\n",
        "        inputs=orig_input2, outputs=DecoderLSTM(EncoderLSTM(orig_input2))\n",
        "    )\n",
        "\n",
        "    return autoencoder2\n",
        "\n"
      ],
      "metadata": {
        "id": "YopwiyfxDr_g"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded.shape"
      ],
      "metadata": {
        "id": "0kU7VTl9Ev-I",
        "outputId": "0ed081ce-557f-4178-e042-f58b8bdc815f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(450, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder(n_timesteps_padded, n_features )\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "3b0127af-2f36-4328-92b5-6b3ad3abc9a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 6s - loss: 0.0616 - val_loss: 0.0144 - 6s/epoch - 377ms/step\n",
            "Epoch 2/50\n",
            "15/15 - 0s - loss: 0.0046 - val_loss: 0.0013 - 394ms/epoch - 26ms/step\n",
            "Epoch 3/50\n",
            "15/15 - 0s - loss: 0.0016 - val_loss: 0.0014 - 339ms/epoch - 23ms/step\n",
            "Epoch 4/50\n",
            "15/15 - 0s - loss: 0.0011 - val_loss: 0.0011 - 416ms/epoch - 28ms/step\n",
            "Epoch 5/50\n",
            "15/15 - 0s - loss: 8.5874e-04 - val_loss: 6.6260e-04 - 367ms/epoch - 24ms/step\n",
            "Epoch 6/50\n",
            "15/15 - 0s - loss: 6.2007e-04 - val_loss: 6.1036e-04 - 390ms/epoch - 26ms/step\n",
            "Epoch 7/50\n",
            "15/15 - 0s - loss: 5.8653e-04 - val_loss: 5.9504e-04 - 356ms/epoch - 24ms/step\n",
            "Epoch 8/50\n",
            "15/15 - 0s - loss: 5.7407e-04 - val_loss: 5.8598e-04 - 379ms/epoch - 25ms/step\n",
            "Epoch 9/50\n",
            "15/15 - 0s - loss: 5.6770e-04 - val_loss: 5.8126e-04 - 396ms/epoch - 26ms/step\n",
            "Epoch 10/50\n",
            "15/15 - 0s - loss: 5.6435e-04 - val_loss: 5.7918e-04 - 405ms/epoch - 27ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.0005791778094135225.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoderLSTM = AutoencoderLSTM(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoderLSTM.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoderLSTM.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ],
      "metadata": {
        "id": "C5Sk6xWIKxMd",
        "outputId": "403e8d4c-b05b-47a7-9eac-24ec6dd0668e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 14s - loss: 0.0197 - val_loss: 0.0089 - 14s/epoch - 904ms/step\n",
            "Epoch 2/50\n",
            "15/15 - 2s - loss: 0.0110 - val_loss: 0.0086 - 2s/epoch - 138ms/step\n",
            "Epoch 3/50\n",
            "15/15 - 2s - loss: 0.0103 - val_loss: 0.0081 - 2s/epoch - 118ms/step\n",
            "Epoch 4/50\n",
            "15/15 - 2s - loss: 0.0098 - val_loss: 0.0079 - 2s/epoch - 105ms/step\n",
            "Epoch 5/50\n",
            "15/15 - 3s - loss: 0.0080 - val_loss: 0.0044 - 3s/epoch - 197ms/step\n",
            "Epoch 6/50\n",
            "15/15 - 3s - loss: 0.0058 - val_loss: 0.0044 - 3s/epoch - 229ms/step\n",
            "Epoch 7/50\n",
            "15/15 - 3s - loss: 0.0049 - val_loss: 0.0042 - 3s/epoch - 205ms/step\n",
            "Epoch 8/50\n",
            "15/15 - 2s - loss: 0.0043 - val_loss: 0.0040 - 2s/epoch - 105ms/step\n",
            "Epoch 9/50\n",
            "15/15 - 1s - loss: 0.0042 - val_loss: 0.0044 - 1s/epoch - 90ms/step\n",
            "Epoch 10/50\n",
            "15/15 - 1s - loss: 0.0041 - val_loss: 0.0034 - 1s/epoch - 92ms/step\n",
            "Epoch 11/50\n",
            "15/15 - 1s - loss: 0.0043 - val_loss: 0.0033 - 1s/epoch - 91ms/step\n",
            "Epoch 12/50\n",
            "15/15 - 1s - loss: 0.0038 - val_loss: 0.0051 - 1s/epoch - 91ms/step\n",
            "Epoch 13/50\n",
            "15/15 - 1s - loss: 0.0050 - val_loss: 0.0040 - 1s/epoch - 91ms/step\n",
            "Epoch 14/50\n",
            "15/15 - 1s - loss: 0.0040 - val_loss: 0.0036 - 1s/epoch - 90ms/step\n",
            "Epoch 15/50\n",
            "15/15 - 2s - loss: 0.0038 - val_loss: 0.0032 - 2s/epoch - 101ms/step\n",
            "Epoch 16/50\n",
            "15/15 - 2s - loss: 0.0037 - val_loss: 0.0033 - 2s/epoch - 131ms/step\n",
            "Epoch 17/50\n",
            "15/15 - 2s - loss: 0.0040 - val_loss: 0.0036 - 2s/epoch - 133ms/step\n",
            "Epoch 18/50\n",
            "15/15 - 2s - loss: 0.0041 - val_loss: 0.0033 - 2s/epoch - 132ms/step\n",
            "Epoch 19/50\n",
            "15/15 - 2s - loss: 0.0037 - val_loss: 0.0034 - 2s/epoch - 118ms/step\n",
            "Epoch 20/50\n",
            "15/15 - 1s - loss: 0.0038 - val_loss: 0.0038 - 1s/epoch - 92ms/step\n",
            "Epoch 21/50\n",
            "15/15 - 1s - loss: 0.0039 - val_loss: 0.0033 - 1s/epoch - 93ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.003235831158235669.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from wildboar.explain import IntervalImportance\n",
        "from LIMESegment.Utils.explanations import LIMESegment\n",
        "def get_global_weight_univariate(\n",
        "    input_samples, input_labels, classifier_model, n_timesteps, n_features, random_state=None,\n",
        "):\n",
        "    n_samples, n_timesteps, n_dims = input_samples.shape  # n_dims=1\n",
        "\n",
        "    class ModelWrapper:\n",
        "        def __init__(self, model, n_timesteps, n_features):\n",
        "            self.model = model\n",
        "            self.fitted_ = False\n",
        "            self.n_timesteps_in_ = n_timesteps\n",
        "            self.n_features_in_ = n_features\n",
        "\n",
        "        def predict(self, X):\n",
        "            p = self.model.predict(X.reshape(n_samples, n_timesteps, 1))\n",
        "            return np.argmax(p, axis=1)\n",
        "\n",
        "        def fit(self, X, y):\n",
        "\n",
        "          self.fitted_ = True\n",
        "          return self.model.fit(X, y)\n",
        "\n",
        "    clf = ModelWrapper(classifier_model, n_timesteps, n_features)\n",
        "\n",
        "    i = IntervalImportance(scoring=\"accuracy\",n_intervals=10, random_state=random_state)\n",
        "    i.fit(clf, input_samples.reshape(input_samples.shape[0], -1), input_labels)\n",
        "\n",
        "    # calculate the threshold of masking, 75 percentile\n",
        "    masking_threshold = np.percentile(i.importances_.mean, 75)\n",
        "    masking_idx = np.where(i.importances_.mean >= masking_threshold)\n",
        "\n",
        "    weighted_steps = np.ones(n_timesteps)\n",
        "    seg_idx = i.components_\n",
        "    for start_idx in masking_idx[0]:\n",
        "        weighted_steps[seg_idx[start_idx][0] : seg_idx[start_idx][1]] = 0\n",
        "\n",
        "    # need to reshape for multiplication in `tf.math.multiply()`\n",
        "    weighted_steps = weighted_steps.reshape(1, n_timesteps, 1)\n",
        "    return weighted_steps"
      ],
      "metadata": {
        "id": "4kov5Co9s9FB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from _guided import get_global_weights\n",
        "PRED_MARGIN_W_LIST = [1]\n",
        "from help_functions import evaluate2\n",
        "w_type = \"global\"\n",
        "\n",
        "if w_type == \"global\":\n",
        "    step_weights = get_global_weightstr(\n",
        "        X_train_processed_padded,\n",
        "        y_train_classes,\n",
        "        classifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "elif w_type == \"uniform\":\n",
        "    step_weights = np.ones((1, n_timesteps_padded, n_features))\n",
        "elif w_type.lower() == \"local\":\n",
        "    step_weights = \"local\"\n",
        "else:\n",
        "    raise NotImplementedError(\n",
        "        \"A.w_type not implemented, please choose 'local', 'global' or 'uniform'.\"\n",
        "    )\n",
        "### Evaluation metrics\n",
        "for pred_margin_weight in PRED_MARGIN_W_LIST:\n",
        "    print(f\"The current prediction margin weight is {pred_margin_weight}.\")\n",
        "\n",
        "    # get the negative predictions, which is class abnormal (0); (normal is class 1)\n",
        "    X_pred_neg = X_test_processed_padded[y_pred_classes == neg_label][:10]\n",
        "    y_pred_neg = y_pred_classes[y_pred_classes==neg_label][:10]\n",
        "    lr_list = [0.0001]\n",
        "    best_lr, best_cf_model, best_cf_samples, _ = find_best_lr(\n",
        "        classifier,\n",
        "        X_pred_neg,\n",
        "        y_pred_neg,\n",
        "        autoencoder=autoencoder,\n",
        "        lr_list=lr_list,\n",
        "        pred_margin_weight=pred_margin_weight,\n",
        "        step_weights=step_weights,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "    print(f\"The best learning rate found is {best_lr}.\")\n",
        "\n",
        "    # predicted probabilities of CFs\n",
        "    z_pred = classifier.predict(best_cf_samples)[:, 1]\n",
        "    if padding_size != 0:\n",
        "        # remove extra paddings after counterfactual generation in 1dCNN autoencoder\n",
        "        best_cf_samples = best_cf_samples[:, :-padding_size, :]\n",
        "        # use the unpadded X for evaluation\n",
        "        X_pred_neg_orignal = X_test_processed[y_pred_classes == neg_label][:10]\n",
        "    else:\n",
        "        X_pred_neg_orignal = X_pred_neg\n",
        "\n",
        "    evaluate_res = evaluate2(\n",
        "        X_pred_neg_orignal, best_cf_samples, z_pred, n_timesteps\n",
        "    )"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h",
        "outputId": "fcd37e48-0a5e-4ab5-e040-954ac9c52108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-ba2a14e1fb40>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"global\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     step_weights = get_global_weightstr(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mX_train_processed_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_train_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-9373664d2beb>\u001b[0m in \u001b[0;36mget_global_weightstr\u001b[0;34m(input_samples, input_labels, classifier_model, n_timesteps, n_features, random_state)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntervalImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_intervals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# calculate the threshold of masking, 75 percentile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wildboar/explain/_importance.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, estimator, x, y, sample_weight)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wildboar/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_timesteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wildboar/base.py\u001b[0m in \u001b[0;36m_check_n_timesteps\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_timesteps\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_timesteps_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"X has {n_timesteps} timesteps, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_timesteps_in_} timesteps as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 5376 timesteps, but IntervalImportance is expecting 896 timesteps as input."
          ]
        }
      ]
    }
  ]
}