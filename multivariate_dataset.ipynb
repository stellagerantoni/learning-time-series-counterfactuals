{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq/t/xJkhLCsLbmV01CVrn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/learning-time-series-counterfactuals/blob/main/multivariate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIiG4khZBur-",
        "outputId": "435df2f3-fed2-450a-f122-bbf2be5e9fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'learning-time-series-counterfactuals' already exists and is not an empty directory.\n",
            "/content/learning-time-series-counterfactuals\n"
          ]
        }
      ],
      "source": [
        " ! git clone https://github.com/stellagerantoni/learning-time-series-counterfactuals\n",
        " %cd learning-time-series-counterfactuals/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon[all_extras]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89L3kts7CCan",
        "outputId": "b9204b74-562d-45f4-9619-e649241444c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aeon[all_extras] in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1.0)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.14)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.56.4)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.1)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.3.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.2.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: filterpy>=1.4.5 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.9.0)\n",
            "Requirement already satisfied: hmmlearn>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: gluonts>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.13.4)\n",
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.51.0)\n",
            "Requirement already satisfied: kotsu>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.3.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.7.1)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.1)\n",
            "Requirement already satisfied: pmdarima<3.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.0.3)\n",
            "Requirement already satisfied: prophet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.4)\n",
            "Requirement already satisfied: pyod>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: scikit-posthocs>=0.6.5 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.7.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.12.2)\n",
            "Requirement already satisfied: statsforecast>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.6.0)\n",
            "Requirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.14.0)\n",
            "Requirement already satisfied: stumpy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.12.0)\n",
            "Requirement already satisfied: tbats>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.3)\n",
            "Requirement already satisfied: tensorflow<2.13.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-probability<0.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.20.1)\n",
            "Requirement already satisfied: tsfresh>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.20.1)\n",
            "Requirement already satisfied: tslearn<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.5.3.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.7.0)\n",
            "Requirement already satisfied: mlflow<2.4.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.3.2)\n",
            "Requirement already satisfied: esig<0.9.8.3,>=0.9.7 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.9.8.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon[all_extras]) (1.14.1)\n",
            "Requirement already satisfied: pydantic~=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (1.10.12)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.66.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (8.1.7)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.17.7)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.35)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.31.0)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.8.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4.4)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (1.12.0)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.1.3)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.2.5)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (1.2.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.0.20)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.4.4)\n",
            "Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (20.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (0.29.36)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima<3.0.0,>=1.8.0->aeon[all_extras]) (1.26.16)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (1.1.0)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.32)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.8.0->aeon[all_extras]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.3.0,>=1.0.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (from statsforecast>=0.5.2->aeon[all_extras]) (0.17.3)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from statsforecast>=0.5.2->aeon[all_extras]) (0.8.6)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.1->aeon[all_extras]) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (1.57.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.13.0->aeon[all_extras]) (0.33.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability<0.21.0->aeon[all_extras]) (0.1.8)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.20.0->aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (1.7.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow<2.4.0->aeon[all_extras]) (1.2.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.13.0->aeon[all_extras]) (0.41.2)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->prophet>=1.1.0->aeon[all_extras]) (0.5.12)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (0.9.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.5)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (6.3.2)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow<2.4.0->aeon[all_extras]) (1.6.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.3.7)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.1.2)\n",
            "Requirement already satisfied: triad>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.9.1)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.2.4)\n",
            "Requirement already satisfied: qpd>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.4.4)\n",
            "Requirement already satisfied: fugue-sql-antlr>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (0.1.6)\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (18.3.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras]) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow<2.4.0->aeon[all_extras]) (3.16.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow<2.13.0->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow<2.4.0->aeon[all_extras]) (2.1.3)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet>=1.1.0->aeon[all_extras]) (4.1.4)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->aeon[all_extras]) (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.4.0->aeon[all_extras]) (2.0.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.7.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime<4.12,>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (4.11.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras]) (5.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (1.3.1)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.10/dist-packages (from triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (2.4.16)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13.0->aeon[all_extras]) (0.5.0)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.1->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "\n",
        "from _composite import ModifiedLatentCF\n",
        "%cd src\n",
        "from _vanilla import LatentCF\n",
        "from help_functions import (ResultWriter, conditional_pad, evaluate,\n",
        "                            find_best_lr, plot_graphs,\n",
        "                            reset_seeds, time_series_normalize,\n",
        "                            time_series_revert, upsample_minority,\n",
        "                            validity_score)\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "cedd3396-066f-45bb-aefb-fcb719cc9e1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/learning-time-series-counterfactuals/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y, meta_data = load_classification(\"SelfRegulationSCP1\")\n",
        "print(\" Shape of X = \", X.shape)\n",
        "print(\" Meta data = \", meta_data)\n",
        "print(X[:3])\n",
        "pos = 'positivity'\n",
        "neg = 'negativity'"
      ],
      "metadata": {
        "id": "GIipsXCr1u3S",
        "outputId": "ef834053-af4d-4b8c-f782-0d2633e56595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (561, 6, 896)\n",
            " Meta data =  {'problemname': 'selfregulationscp1', 'timestamps': False, 'missing': False, 'univariate': False, 'equallength': True, 'classlabel': True, 'targetlabel': False, 'class_values': ['negativity', 'positivity']}\n",
            "[[[ 23.    21.66  20.84 ...  21.84  22.62  21.5 ]\n",
            "  [ 19.03  19.19  21.5  ...  27.03  26.78  28.94]\n",
            "  [ 32.19  36.81  40.16 ...  32.5   34.53  37.12]\n",
            "  [ 43.66  41.22  39.69 ...  50.06  50.84  50.75]\n",
            "  [ 30.72  31.81  31.69 ...  44.69  43.69  41.88]\n",
            "  [ 39.09  39.53  40.59 ...  44.03  44.75  46.19]]\n",
            "\n",
            " [[ 29.62  29.    28.66 ...  42.09  38.    34.97]\n",
            "  [ 27.19  27.56  27.91 ... -10.78 -10.28  -9.03]\n",
            "  [ 27.94  26.19  23.94 ...  -3.97  -2.66  -2.38]\n",
            "  [ 19.38  20.66  22.78 ...  -4.84  -5.47  -5.78]\n",
            "  [ 35.44  38.81  40.41 ...  -3.66  -1.84   1.62]\n",
            "  [ 32.25  30.16  29.44 ...  12.88  11.81   9.75]]\n",
            "\n",
            " [[ 24.41  25.12  26.06 ...  23.88  23.94  24.03]\n",
            "  [ 29.41  30.31  30.53 ... -14.19 -12.78 -12.38]\n",
            "  [ 28.69  24.62  21.72 ...  -8.16  -8.06  -9.22]\n",
            "  [ 15.31  15.41  17.44 ... -21.78 -24.78 -25.66]\n",
            "  [ 20.5   20.66  21.38 ... -22.72 -22.66 -22.84]\n",
            "  [ 28.06  29.91  31.09 ... -19.59 -16.66 -13.31]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X, y, meta_data = load_classification(\"Heartbeat\")\n",
        "print(\" Shape of X = \", X.shape)\n",
        "print(\" Meta data = \", meta_data)\n",
        "print(X[:3])\n",
        "pos = 'normal'\n",
        "neg = 'abnormal'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsJJktx22dXs",
        "outputId": "ef82d14a-4f03-4d2a-b677-997f04d820be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (409, 61, 405)\n",
            " Meta data =  {'problemname': 'heartbeat', 'timestamps': False, 'missing': False, 'univariate': False, 'equallength': True, 'classlabel': True, 'targetlabel': False, 'class_values': ['normal', 'abnormal']}\n",
            "[[[9.4900e-04 1.4880e-03 3.1400e-04 ... 8.0400e-04 8.1500e-04 1.3890e-03]\n",
            "  [1.2880e-03 1.1400e-03 4.3000e-04 ... 1.3640e-03 3.6300e-04 8.4300e-04]\n",
            "  [5.2900e-04 1.6350e-03 2.1460e-03 ... 1.1790e-03 3.6300e-04 5.0400e-04]\n",
            "  ...\n",
            "  [7.8312e-02 1.4568e-01 2.3805e-01 ... 4.4070e-01 6.3277e-01 6.5026e-01]\n",
            "  [4.5608e-02 1.1980e-01 1.7813e-01 ... 3.7934e-01 3.7387e-01 3.2700e-01]\n",
            "  [1.2107e-01 1.3385e-01 4.0077e-02 ... 9.8976e-02 2.5955e-02 3.6082e-02]]\n",
            "\n",
            " [[2.0264e-02 1.7023e-02 6.0520e-03 ... 5.3840e-03 8.3720e-03 7.8450e-03]\n",
            "  [2.7680e-02 3.6928e-02 3.4255e-02 ... 1.5869e-02 8.6480e-03 2.5470e-03]\n",
            "  [2.3423e-02 4.1957e-02 3.4538e-02 ... 9.0860e-03 9.5190e-03 7.3490e-03]\n",
            "  ...\n",
            "  [7.8027e-01 4.4498e-01 4.9525e-01 ... 9.8392e-02 2.0994e-01 4.6098e-01]\n",
            "  [5.4370e-01 1.6080e-01 5.4133e-01 ... 1.0935e-01 2.5578e-01 4.1651e-01]\n",
            "  [2.4570e-03 5.0050e-01 7.2808e-01 ... 1.8733e-01 5.0635e-02 1.7935e-01]]\n",
            "\n",
            " [[2.1082e-02 2.0687e-02 9.9480e-03 ... 1.9040e-03 1.0879e-02 3.4410e-03]\n",
            "  [1.9906e-02 1.9558e-02 6.6820e-03 ... 1.1220e-03 9.0720e-03 5.7920e-03]\n",
            "  [1.9319e-02 1.9314e-02 5.6700e-03 ... 8.1800e-04 9.8380e-03 5.0670e-03]\n",
            "  ...\n",
            "  [1.9674e-01 1.7592e-01 2.7098e-01 ... 1.0278e+00 1.6183e+00 6.9665e-01]\n",
            "  [3.4641e-01 3.1551e-01 3.2801e-01 ... 9.9148e-01 1.2083e+00 1.8203e+00]\n",
            "  [1.4322e-01 4.2338e-01 3.7645e-01 ... 1.4404e-01 5.8677e-02 2.3513e-01]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert positive and negative labels to 1 and 0\n",
        "pos_label, neg_label = 1, 0\n",
        "y_copy = y.copy()\n",
        "if pos != pos_label:\n",
        "    y_copy[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "if neg != neg_label:\n",
        "    y_copy[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "y_copy = y_copy.astype(int)"
      ],
      "metadata": {
        "id": "YLeLFhMcDJJJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_copy, test_size=0.2, random_state=RANDOM_STATE, stratify=y)"
      ],
      "metadata": {
        "id": "-2BhRnuMBM3k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_counts = pd.value_counts(y)[pos_label]\n",
        "neg_counts = pd.value_counts(y)[neg_label]"
      ],
      "metadata": {
        "id": "LCroXlO3fvFE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_counts)\n",
        "print(neg_counts)"
      ],
      "metadata": {
        "id": "q9H6bBnpf1CR",
        "outputId": "f4d7e149-67fe-495c-9b0f-2ca1bae3200a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "279\n",
            "282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample, shuffle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def downsample_multivariate_time_series(X, y, pos_label=1, neg_label=0, random_state=39):\n",
        "    # Get counts\n",
        "    pos_counts = pd.value_counts(y)[pos_label]\n",
        "    neg_counts = pd.value_counts(y)[neg_label]\n",
        "\n",
        "    # Divide by class\n",
        "    X_pos, X_neg = X[y == pos_label], X[y == neg_label]\n",
        "\n",
        "    if pos_counts == neg_counts:\n",
        "        # Balanced dataset\n",
        "        return X, y\n",
        "    elif pos_counts < neg_counts:\n",
        "        # Downsample majority class\n",
        "        X_neg_down = resample(\n",
        "            X_neg, replace=False, n_samples=pos_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos, X_neg_down], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(pos_counts)]\n",
        "            + [neg_label for j in range(pos_counts)]\n",
        "        )\n",
        "    else:\n",
        "        # Downsample majority class\n",
        "        X_pos_down = resample(\n",
        "            X_pos, replace=False, n_samples=neg_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos_down, X_neg], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(neg_counts)]\n",
        "            + [neg_label for j in range(neg_counts)]\n",
        "        )\n",
        "\n",
        "    # Shuffle the index after down-sampling\n",
        "    X_concat, y_concat = shuffle(X_concat, y_concat, random_state=random_state)\n",
        "\n",
        "    return X_concat, y_concat"
      ],
      "metadata": {
        "id": "cQoJ28egjpPE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_majority(X, y, pos_label=1, neg_label=0, random_state=39):\n",
        "    # Get counts\n",
        "    pos_counts = pd.value_counts(y)[pos_label]\n",
        "    neg_counts = pd.value_counts(y)[neg_label]\n",
        "\n",
        "    # Divide by class\n",
        "    X_pos, X_neg = X[y == pos_label], X[y == neg_label]\n",
        "\n",
        "    if pos_counts == neg_counts:\n",
        "        # Balanced dataset\n",
        "        return X, y\n",
        "    elif pos_counts < neg_counts:\n",
        "        # Downsample majority class\n",
        "        X_neg_down = resample(\n",
        "            X_neg, replace=False, n_samples=pos_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos, X_neg_down], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(pos_counts)]\n",
        "            + [neg_label for j in range(pos_counts)]\n",
        "        )\n",
        "    else:\n",
        "        # Downsample majority class\n",
        "        X_pos_down = resample(\n",
        "            X_pos, replace=False, n_samples=neg_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos_down, X_neg], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(neg_counts)]\n",
        "            + [neg_label for j in range(neg_counts)]\n",
        "        )\n",
        "\n",
        "    # Shuffle the index after down-sampling\n",
        "    X_concat, y_concat = shuffle(X_concat, y_concat, random_state=random_state)\n",
        "\n",
        "    return X_concat, y_concat"
      ],
      "metadata": {
        "id": "AvcDFFcimI2q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample, shuffle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def upsample_multivariate_time_series(X, y, pos_label=1, neg_label=0, random_state=39):\n",
        "    # Ensure that the data is in the right format\n",
        "    if len(X.shape) != 3:\n",
        "        raise ValueError(\"X should be a 3D array: [samples, features, time_steps].\")\n",
        "\n",
        "    # Convert y to numpy array for indexing\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Get counts\n",
        "    pos_counts = np.sum(y == pos_label)\n",
        "    neg_counts = np.sum(y == neg_label)\n",
        "\n",
        "    # Split data by class\n",
        "    X_pos, X_neg = X[y == pos_label], X[y == neg_label]\n",
        "\n",
        "    if pos_counts == neg_counts:\n",
        "        # Balanced dataset\n",
        "        return X, y\n",
        "    elif pos_counts > neg_counts:\n",
        "        # Imbalanced dataset\n",
        "        X_neg_over = resample(\n",
        "            X_neg, replace=True, n_samples=pos_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos, X_neg_over], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(pos_counts)]\n",
        "            + [neg_label for j in range(pos_counts)]\n",
        "        )\n",
        "    else:\n",
        "        # Imbalanced dataset\n",
        "        X_pos_over = resample(\n",
        "            X_pos, replace=True, n_samples=neg_counts, random_state=random_state\n",
        "        )\n",
        "        X_concat = np.concatenate([X_pos_over, X_neg], axis=0)\n",
        "        y_concat = np.array(\n",
        "            [pos_label for i in range(neg_counts)]\n",
        "            + [neg_label for j in range(neg_counts)]\n",
        "        )\n",
        "\n",
        "    # Shuffle the upsampled dataset\n",
        "    shuffled_indices = np.arange(X_concat.shape[0])\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "\n",
        "    X_concat = X_concat[shuffled_indices]\n",
        "    y_concat = y_concat[shuffled_indices]\n",
        "\n",
        "    return X_concat, y_concat"
      ],
      "metadata": {
        "id": "SuvrGsGkeXgY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsample the minority class\n",
        "\n",
        "pos_counts = pd.value_counts(y_train)[pos_label]\n",
        "neg_counts = pd.value_counts(y_train)[neg_label]\n",
        "print(f\"negative_count = {neg_counts}, positive_count = {pos_counts}\")\n",
        "\n",
        "if pos_counts!=neg_counts:\n",
        "  X_train, y_train = upsample_multivariate_time_series(X_train, y_train, pos_label=pos_label, neg_label=neg_label)\n",
        "  print(f\"Data upsampling performed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n",
        "else:\n",
        "   print(f\"Data upsampling not needed, current distribution of y: \\n{pd.value_counts(y_train)}.\")\n"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "672d6677-a7a5-45ef-95da-031e95995d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative_count = 225, positive_count = 223\n",
            "Data upsampling performed, current distribution of y: \n",
            "1    225\n",
            "0    225\n",
            "dtype: int64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_counts = pd.value_counts(y_train)[pos_label]\n",
        "neg_counts = pd.value_counts(y_train)[neg_label]"
      ],
      "metadata": {
        "id": "aHTGYkiEWEA7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_counts)\n",
        "neg_counts"
      ],
      "metadata": {
        "id": "OEbakRckWHo-",
        "outputId": "530e2786-dbeb-4335-c036-7fa4f7f520df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "225"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "def time_series_normalize_multivariate(data, n_timesteps, n_features, scaler=None):\n",
        "    # First transpose the data to have shape (samples, timesteps, features)\n",
        "    data_transposed = np.transpose(data, (0, 2, 1))\n",
        "\n",
        "    # Then reshape data to have timesteps as rows for normalization\n",
        "    data_reshaped = data_transposed.reshape(-1, n_features)\n",
        "\n",
        "    if scaler is None:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaler.fit(data_reshaped)\n",
        "\n",
        "    normalized = scaler.transform(data_reshaped)\n",
        "\n",
        "    # Return data reshaped\n",
        "    normalized_transposed = normalized.reshape(-1, n_timesteps, n_features)\n",
        "    return np.transpose(normalized_transposed, (0, 2, 1)), scaler"
      ],
      "metadata": {
        "id": "5i02pFMX7kNe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_pad_multivariate(X):\n",
        "    num_timesteps = X.shape[2]\n",
        "\n",
        "    if num_timesteps % 4 != 0:\n",
        "        next_num = (int(num_timesteps / 4) + 1) * 4\n",
        "        padding_size = next_num - num_timesteps\n",
        "        X_padded = np.pad(\n",
        "            X, pad_width=((0, 0), (0, 0), (0, padding_size))\n",
        "        )\n",
        "\n",
        "        return X_padded, padding_size\n",
        "\n",
        "    return X, 0"
      ],
      "metadata": {
        "id": "exXsbobMPNne"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_training, n_features, n_timesteps = X_train.shape\n",
        "\n",
        "X_train_processed, trained_scaler =  time_series_normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  time_series_normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[2]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "e8e0d49f-387a-4496-88d0-0dfe04fefbd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=896, padded #timesteps=896.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check that padding paddes the right dimention\n",
        "# The timesteps(3rd dimention) should have changed if padded was needed\n",
        "print(f\"X_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")"
      ],
      "metadata": {
        "id": "kvHkoMA0KSgF",
        "outputId": "ab820d21-e781-4535-aade-3014aa5b45b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape = (450, 6, 896)\n",
            "X_train_processed_padded.shape = (450, 6, 896)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"min value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")"
      ],
      "metadata": {
        "id": "1jcRscAbSUfE",
        "outputId": "8295ce15-593f-4594-dc60-3ee49b983962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min value = -73.84, max value = 175.06\n",
            "min value normalized = 0.0, max value normalized= 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_classes = y_train\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))"
      ],
      "metadata": {
        "id": "XHvhyShuvFuO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(X_train_processed), np.max(X_train_processed))\n",
        "print(np.min(X_train), np.max(X_train))"
      ],
      "metadata": {
        "id": "vaDfc98sxJLd",
        "outputId": "099d2fef-c5b8-47f3-f450-1fec28fcc983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 1.0\n",
            "-73.84 175.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=1\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    inputs = keras.Input(shape=(n_features, n_timesteps), dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "8v2HDOL6WUbA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_counts = pd.value_counts(y_train_classes)[pos_label]\n",
        "neg_counts = pd.value_counts(y_train_classes)[neg_label]"
      ],
      "metadata": {
        "id": "pQE69tC68qoa"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_counts)\n",
        "print(neg_counts)"
      ],
      "metadata": {
        "id": "WOM61Dam9ADI",
        "outputId": "374d66b2-f2e6-414c-afbb-40b85439e7cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "225\n",
            "225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=1\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "    inputs = keras.Input(shape=(n_features, n_timesteps), dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128)(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "def LSTMFCNClassifier(n_timesteps, n_features, n_output=2, n_LSTM_cells=8, regularization_rate = 0.001):\n",
        "    # https://github.com/titu1994/LSTM-FCN/blob/master/hyperparameter_search.py\n",
        "    inputs = keras.Input(shape=( n_features,n_timesteps), dtype=\"float32\")\n",
        "\n",
        "    x = keras.layers.LSTM(units=n_LSTM_cells, kernel_regularizer=l2(regularization_rate))(inputs)\n",
        "    x = keras.layers.Dropout(rate=0.8)(x)\n",
        "\n",
        "    y = keras.layers.Permute((2, 1))(inputs)\n",
        "    y = keras.layers.Conv1D(64, 8, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(128, 5, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.Conv1D(64, 3, padding=\"same\", kernel_initializer=\"he_uniform\", kernel_regularizer=l2(regularization_rate))(y)\n",
        "    y = keras.layers.BatchNormalization()(y)\n",
        "    y = keras.layers.ReLU()(y)\n",
        "\n",
        "    y = keras.layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    x = keras.layers.concatenate([x, y])\n",
        "\n",
        "    outputs = keras.layers.Dense(n_output, activation=\"softmax\", kernel_regularizer=l2(regularization_rate))(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def ClassifierLSTM(n_timesteps, n_features, extra_lstm_layer=True, n_output=1):\n",
        "    # Define the model structure - only LSTM layers\n",
        "    # https://www.kaggle.com/szaitseff/classification-of-time-series-with-lstm-rnn\n",
        "    inputs = keras.Input(shape=(n_features, n_timesteps), dtype=\"float32\")\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(\n",
        "            inputs\n",
        "        )  # set return_sequences true to feed next LSTM layer\n",
        "    else:\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(\n",
        "            inputs\n",
        "        )  # set return_sequences false to feed dense layer directly\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    # x = keras.layers.LSTM(32, activation='tanh', return_sequences=True)(x)\n",
        "    # x = keras.layers.BatchNormalization()(x)\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(16, activation=\"tanh\", return_sequences=False)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier2 = keras.Model(inputs, outputs)\n",
        "\n",
        "    return classifier2\n",
        "\n",
        "def Classifier_FCN(input_shape, nb_classes):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding=\"same\")(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation=\"relu\")(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"same\")(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation(\"relu\")(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3, padding=\"same\")(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation(\"relu\")(conv3)\n",
        "\n",
        "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(nb_classes, activation=\"softmax\")(gap_layer)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sD3qinpSEkWK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_lstmcells = 8\n",
        "\n",
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ## 2.0 LSTM-FCN classifier\n",
        "###############################################\n",
        "# ### LSTM-FCN classifier\n",
        "classifier = LSTMFCNClassifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, n_LSTM_cells=n_lstmcells\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "c3506546-bda6-481b-983b-0bdab9044d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "15/15 [==============================] - 12s 528ms/step - loss: 1.0955 - accuracy: 0.7756 - val_loss: 1.2341 - val_accuracy: 0.5044\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 7s 464ms/step - loss: 1.0301 - accuracy: 0.7556 - val_loss: 1.2126 - val_accuracy: 0.5044\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.9674 - accuracy: 0.7889 - val_loss: 1.1974 - val_accuracy: 0.5044\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 8s 577ms/step - loss: 0.9449 - accuracy: 0.7733 - val_loss: 1.1701 - val_accuracy: 0.5044\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.8965 - accuracy: 0.7844 - val_loss: 1.1177 - val_accuracy: 0.5221\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 6s 414ms/step - loss: 0.8457 - accuracy: 0.8000 - val_loss: 1.0742 - val_accuracy: 0.5398\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 8s 500ms/step - loss: 0.8240 - accuracy: 0.7867 - val_loss: 1.0186 - val_accuracy: 0.5575\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 5s 354ms/step - loss: 0.8186 - accuracy: 0.7622 - val_loss: 0.9634 - val_accuracy: 0.5575\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 9s 599ms/step - loss: 0.7936 - accuracy: 0.7889 - val_loss: 0.9119 - val_accuracy: 0.6991\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 6s 372ms/step - loss: 0.7531 - accuracy: 0.7911 - val_loss: 0.8817 - val_accuracy: 0.7257\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 6s 371ms/step - loss: 0.7285 - accuracy: 0.7822 - val_loss: 0.8606 - val_accuracy: 0.7965\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 8s 535ms/step - loss: 0.7242 - accuracy: 0.7889 - val_loss: 0.8310 - val_accuracy: 0.7257\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 5s 356ms/step - loss: 0.7082 - accuracy: 0.7889 - val_loss: 0.7763 - val_accuracy: 0.7788\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 8s 511ms/step - loss: 0.6955 - accuracy: 0.7933 - val_loss: 0.7842 - val_accuracy: 0.7168\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 6s 410ms/step - loss: 0.6569 - accuracy: 0.7889 - val_loss: 0.7764 - val_accuracy: 0.7080\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 5s 358ms/step - loss: 0.6712 - accuracy: 0.7889 - val_loss: 0.8661 - val_accuracy: 0.5752\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 8s 571ms/step - loss: 0.6578 - accuracy: 0.7889 - val_loss: 0.7856 - val_accuracy: 0.6726\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 5s 359ms/step - loss: 0.6450 - accuracy: 0.7889 - val_loss: 0.7936 - val_accuracy: 0.6460\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.6308 - accuracy: 0.8089 - val_loss: 1.0495 - val_accuracy: 0.5133\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 7s 439ms/step - loss: 0.6182 - accuracy: 0.7844 - val_loss: 0.8370 - val_accuracy: 0.5664\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 5s 362ms/step - loss: 0.6065 - accuracy: 0.7956 - val_loss: 0.6930 - val_accuracy: 0.7168\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 8s 580ms/step - loss: 0.6039 - accuracy: 0.8044 - val_loss: 0.8079 - val_accuracy: 0.6195\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 5s 361ms/step - loss: 0.6101 - accuracy: 0.7956 - val_loss: 1.0753 - val_accuracy: 0.5133\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 7s 465ms/step - loss: 0.5953 - accuracy: 0.8022 - val_loss: 1.0098 - val_accuracy: 0.6195\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 7s 455ms/step - loss: 0.5768 - accuracy: 0.8222 - val_loss: 0.6922 - val_accuracy: 0.7965\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 5s 364ms/step - loss: 0.6038 - accuracy: 0.7800 - val_loss: 0.8030 - val_accuracy: 0.4956\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 8s 573ms/step - loss: 0.5802 - accuracy: 0.8178 - val_loss: 0.8581 - val_accuracy: 0.5841\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 5s 363ms/step - loss: 0.5746 - accuracy: 0.7956 - val_loss: 1.1725 - val_accuracy: 0.4956\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 6s 424ms/step - loss: 0.5705 - accuracy: 0.8111 - val_loss: 0.9394 - val_accuracy: 0.5310\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.5863 - accuracy: 0.7756 - val_loss: 0.6314 - val_accuracy: 0.7965\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 5s 357ms/step - loss: 0.5611 - accuracy: 0.8067 - val_loss: 0.6487 - val_accuracy: 0.7257\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 8s 558ms/step - loss: 0.5432 - accuracy: 0.8044 - val_loss: 0.6079 - val_accuracy: 0.8142\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 6s 360ms/step - loss: 0.5502 - accuracy: 0.8000 - val_loss: 0.7250 - val_accuracy: 0.7965\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 6s 379ms/step - loss: 0.5281 - accuracy: 0.8000 - val_loss: 0.8592 - val_accuracy: 0.4956\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 8s 539ms/step - loss: 0.5521 - accuracy: 0.8022 - val_loss: 0.8796 - val_accuracy: 0.6372\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 5s 358ms/step - loss: 0.5217 - accuracy: 0.8089 - val_loss: 0.8128 - val_accuracy: 0.7788\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 8s 519ms/step - loss: 0.5209 - accuracy: 0.8133 - val_loss: 0.9208 - val_accuracy: 0.4956\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 6s 394ms/step - loss: 0.5280 - accuracy: 0.8044 - val_loss: 0.6961 - val_accuracy: 0.6460\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 5s 356ms/step - loss: 0.5436 - accuracy: 0.8156 - val_loss: 0.5998 - val_accuracy: 0.7434\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 8s 576ms/step - loss: 0.5303 - accuracy: 0.8422 - val_loss: 0.9306 - val_accuracy: 0.7257\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 8s 536ms/step - loss: 0.5203 - accuracy: 0.8044 - val_loss: 0.6543 - val_accuracy: 0.6637\n",
            "Epoch 42/150\n",
            " 7/15 [=============>................] - ETA: 8s - loss: 0.5185 - accuracy: 0.8259"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ce8ca010f70b>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mreset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training log for LSTM-FCN classifier:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m classifier_history = classifier.fit(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mX_train_processed_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### LSTM classifier\n",
        "classifier = ClassifierLSTM(\n",
        "    n_timesteps_padded, n_features,extra_lstm_layer=False, n_output=2\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=30, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=42,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, y_test),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:pos\", \"True:neg\"],\n",
        "    columns=[\"Pred:pos\", \"Pred:neg\"],\n",
        ")\n",
        "print(f\"Confusion matrix: \\n{confusion_matrix_df}.\")"
      ],
      "metadata": {
        "id": "fvEe9-iRwX-h",
        "outputId": "c271390a-cda2-4a65-970f-2c2a58f60f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM classifier:\n",
            "Epoch 1/150\n",
            "11/11 - 13s - loss: 0.4700 - accuracy: 0.7689 - val_loss: 0.7359 - val_accuracy: 0.5044 - 13s/epoch - 1s/step\n",
            "Epoch 2/150\n",
            "11/11 - 0s - loss: 0.3803 - accuracy: 0.8422 - val_loss: 0.6454 - val_accuracy: 0.5487 - 276ms/epoch - 25ms/step\n",
            "Epoch 3/150\n",
            "11/11 - 0s - loss: 0.3429 - accuracy: 0.8689 - val_loss: 0.5992 - val_accuracy: 0.6814 - 181ms/epoch - 16ms/step\n",
            "Epoch 4/150\n",
            "11/11 - 0s - loss: 0.3466 - accuracy: 0.8422 - val_loss: 0.5871 - val_accuracy: 0.7345 - 186ms/epoch - 17ms/step\n",
            "Epoch 5/150\n",
            "11/11 - 0s - loss: 0.3122 - accuracy: 0.8756 - val_loss: 0.5892 - val_accuracy: 0.7080 - 171ms/epoch - 16ms/step\n",
            "Epoch 6/150\n",
            "11/11 - 0s - loss: 0.2957 - accuracy: 0.8711 - val_loss: 0.5974 - val_accuracy: 0.6372 - 299ms/epoch - 27ms/step\n",
            "Epoch 7/150\n",
            "11/11 - 0s - loss: 0.2767 - accuracy: 0.8867 - val_loss: 0.5718 - val_accuracy: 0.8850 - 275ms/epoch - 25ms/step\n",
            "Epoch 8/150\n",
            "11/11 - 0s - loss: 0.2931 - accuracy: 0.8889 - val_loss: 0.5848 - val_accuracy: 0.7876 - 306ms/epoch - 28ms/step\n",
            "Epoch 9/150\n",
            "11/11 - 0s - loss: 0.2985 - accuracy: 0.8756 - val_loss: 0.5651 - val_accuracy: 0.8053 - 258ms/epoch - 23ms/step\n",
            "Epoch 10/150\n",
            "11/11 - 0s - loss: 0.2470 - accuracy: 0.8889 - val_loss: 0.5694 - val_accuracy: 0.7611 - 362ms/epoch - 33ms/step\n",
            "Epoch 11/150\n",
            "11/11 - 0s - loss: 0.2371 - accuracy: 0.8911 - val_loss: 0.6526 - val_accuracy: 0.5398 - 331ms/epoch - 30ms/step\n",
            "Epoch 12/150\n",
            "11/11 - 0s - loss: 0.2399 - accuracy: 0.9133 - val_loss: 0.5147 - val_accuracy: 0.8584 - 296ms/epoch - 27ms/step\n",
            "Epoch 13/150\n",
            "11/11 - 0s - loss: 0.2273 - accuracy: 0.8978 - val_loss: 0.7470 - val_accuracy: 0.4956 - 302ms/epoch - 27ms/step\n",
            "Epoch 14/150\n",
            "11/11 - 0s - loss: 0.2142 - accuracy: 0.9222 - val_loss: 0.5166 - val_accuracy: 0.7965 - 266ms/epoch - 24ms/step\n",
            "Epoch 15/150\n",
            "11/11 - 0s - loss: 0.1978 - accuracy: 0.9222 - val_loss: 0.8866 - val_accuracy: 0.4956 - 284ms/epoch - 26ms/step\n",
            "Epoch 16/150\n",
            "11/11 - 0s - loss: 0.2283 - accuracy: 0.9133 - val_loss: 0.6059 - val_accuracy: 0.5487 - 273ms/epoch - 25ms/step\n",
            "Epoch 17/150\n",
            "11/11 - 0s - loss: 0.2206 - accuracy: 0.9156 - val_loss: 0.7544 - val_accuracy: 0.4956 - 284ms/epoch - 26ms/step\n",
            "Epoch 18/150\n",
            "11/11 - 0s - loss: 0.1923 - accuracy: 0.9378 - val_loss: 0.5143 - val_accuracy: 0.8319 - 316ms/epoch - 29ms/step\n",
            "Epoch 19/150\n",
            "11/11 - 0s - loss: 0.2195 - accuracy: 0.9156 - val_loss: 0.8779 - val_accuracy: 0.4956 - 277ms/epoch - 25ms/step\n",
            "Epoch 20/150\n",
            "11/11 - 0s - loss: 0.1849 - accuracy: 0.9289 - val_loss: 0.5393 - val_accuracy: 0.7345 - 311ms/epoch - 28ms/step\n",
            "Epoch 21/150\n",
            "11/11 - 0s - loss: 0.1897 - accuracy: 0.9244 - val_loss: 0.4553 - val_accuracy: 0.8053 - 297ms/epoch - 27ms/step\n",
            "Epoch 22/150\n",
            "11/11 - 0s - loss: 0.1709 - accuracy: 0.9311 - val_loss: 0.8597 - val_accuracy: 0.5044 - 308ms/epoch - 28ms/step\n",
            "Epoch 23/150\n",
            "11/11 - 0s - loss: 0.1834 - accuracy: 0.9400 - val_loss: 0.8789 - val_accuracy: 0.5133 - 309ms/epoch - 28ms/step\n",
            "Epoch 24/150\n",
            "11/11 - 0s - loss: 0.1664 - accuracy: 0.9422 - val_loss: 0.4141 - val_accuracy: 0.8142 - 272ms/epoch - 25ms/step\n",
            "Epoch 25/150\n",
            "11/11 - 0s - loss: 0.1816 - accuracy: 0.9267 - val_loss: 0.6733 - val_accuracy: 0.5664 - 313ms/epoch - 28ms/step\n",
            "Epoch 26/150\n",
            "11/11 - 0s - loss: 0.1959 - accuracy: 0.9356 - val_loss: 1.0187 - val_accuracy: 0.5133 - 302ms/epoch - 27ms/step\n",
            "Epoch 27/150\n",
            "11/11 - 0s - loss: 0.1563 - accuracy: 0.9400 - val_loss: 0.4087 - val_accuracy: 0.8673 - 320ms/epoch - 29ms/step\n",
            "Epoch 28/150\n",
            "11/11 - 0s - loss: 0.1369 - accuracy: 0.9556 - val_loss: 0.6550 - val_accuracy: 0.5929 - 210ms/epoch - 19ms/step\n",
            "Epoch 29/150\n",
            "11/11 - 0s - loss: 0.1345 - accuracy: 0.9578 - val_loss: 0.5822 - val_accuracy: 0.6283 - 174ms/epoch - 16ms/step\n",
            "Epoch 30/150\n",
            "11/11 - 0s - loss: 0.1500 - accuracy: 0.9422 - val_loss: 0.3735 - val_accuracy: 0.8673 - 170ms/epoch - 15ms/step\n",
            "Epoch 31/150\n",
            "11/11 - 0s - loss: 0.1944 - accuracy: 0.9200 - val_loss: 0.4054 - val_accuracy: 0.8761 - 171ms/epoch - 16ms/step\n",
            "Epoch 32/150\n",
            "11/11 - 0s - loss: 0.1947 - accuracy: 0.9289 - val_loss: 0.6075 - val_accuracy: 0.6814 - 168ms/epoch - 15ms/step\n",
            "Epoch 33/150\n",
            "11/11 - 0s - loss: 0.1476 - accuracy: 0.9422 - val_loss: 3.7126 - val_accuracy: 0.4956 - 163ms/epoch - 15ms/step\n",
            "Epoch 34/150\n",
            "11/11 - 0s - loss: 0.1539 - accuracy: 0.9400 - val_loss: 0.4378 - val_accuracy: 0.7876 - 164ms/epoch - 15ms/step\n",
            "Epoch 35/150\n",
            "11/11 - 0s - loss: 0.1499 - accuracy: 0.9444 - val_loss: 1.4503 - val_accuracy: 0.5133 - 176ms/epoch - 16ms/step\n",
            "Epoch 36/150\n",
            "11/11 - 0s - loss: 0.1394 - accuracy: 0.9533 - val_loss: 0.9793 - val_accuracy: 0.5310 - 170ms/epoch - 15ms/step\n",
            "Epoch 37/150\n",
            "11/11 - 0s - loss: 0.1503 - accuracy: 0.9444 - val_loss: 2.1907 - val_accuracy: 0.4956 - 183ms/epoch - 17ms/step\n",
            "4/4 [==============================] - 1s 7ms/step\n",
            "LSTM classifier trained, with validation accuracy 0.8850250626566416.\n",
            "Confusion matrix: \n",
            "          Pred:pos  Pred:neg\n",
            "True:pos        50         6\n",
            "True:neg         7        50.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shallow_cnn = False\n",
        "# ### 1dCNN classifier\n",
        "if shallow_cnn == True:\n",
        "    print(f\"Check shallow_cnn argument={shallow_cnn}, use the shallow structure.\")\n",
        "    classifier = Classifier(n_timesteps_padded, n_features) # shallow CNN for small data size\n",
        "else:\n",
        "    classifier = Classifier(n_timesteps_padded, n_features, n_conv_layers=3, add_dense_layer=False) # deeper CNN layers for data with larger size\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "classifier_history = classifier.fit(X_train_processed_padded,\n",
        "        y_train_classes,\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        shuffle=True,\n",
        "        verbose=True,\n",
        "        validation_data=(X_test_processed_padded, y_test_classes),\n",
        "        callbacks=[early_stopping_accuracy])\n",
        "\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "# y_pred_classes = np.array([1 if pred > 0.5 else 0 for pred in y_pred])\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"1dCNN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "        confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "        index=['True:pos', 'True:neg'],\n",
        "        columns=['Pred:pos', 'Pred:neg']\n",
        "    )\n",
        "confusion_matrix_df"
      ],
      "metadata": {
        "id": "GBp7UTpPL7cY",
        "outputId": "c12a6914-7944-4d5b-f8f9-345df1eb1e7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "15/15 [==============================] - 3s 35ms/step - loss: 0.5416 - accuracy: 0.7622 - val_loss: 0.6624 - val_accuracy: 0.5133\n",
            "Epoch 2/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.4217 - accuracy: 0.8133 - val_loss: 0.6622 - val_accuracy: 0.5221\n",
            "Epoch 3/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3188 - accuracy: 0.8711 - val_loss: 0.6371 - val_accuracy: 0.5487\n",
            "Epoch 4/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3593 - accuracy: 0.8378 - val_loss: 0.6095 - val_accuracy: 0.5929\n",
            "Epoch 5/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3774 - accuracy: 0.8511 - val_loss: 1.6900 - val_accuracy: 0.5133\n",
            "Epoch 6/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3351 - accuracy: 0.8489 - val_loss: 0.5945 - val_accuracy: 0.6372\n",
            "Epoch 7/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2872 - accuracy: 0.8822 - val_loss: 0.5388 - val_accuracy: 0.7168\n",
            "Epoch 8/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2672 - accuracy: 0.8956 - val_loss: 0.6555 - val_accuracy: 0.5044\n",
            "Epoch 9/150\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3310 - accuracy: 0.8467 - val_loss: 0.5477 - val_accuracy: 0.7080\n",
            "Epoch 10/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2347 - accuracy: 0.8978 - val_loss: 0.6033 - val_accuracy: 0.6726\n",
            "Epoch 11/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.2371 - accuracy: 0.9000 - val_loss: 0.4499 - val_accuracy: 0.8761\n",
            "Epoch 12/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3183 - accuracy: 0.8711 - val_loss: 0.6931 - val_accuracy: 0.5310\n",
            "Epoch 13/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2930 - accuracy: 0.8689 - val_loss: 0.7663 - val_accuracy: 0.6106\n",
            "Epoch 14/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2430 - accuracy: 0.8933 - val_loss: 0.8946 - val_accuracy: 0.5929\n",
            "Epoch 15/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2005 - accuracy: 0.9067 - val_loss: 1.0472 - val_accuracy: 0.5752\n",
            "Epoch 16/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2289 - accuracy: 0.9022 - val_loss: 0.8229 - val_accuracy: 0.6460\n",
            "Epoch 17/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2625 - accuracy: 0.8956 - val_loss: 0.9855 - val_accuracy: 0.5929\n",
            "Epoch 18/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1927 - accuracy: 0.9244 - val_loss: 0.4273 - val_accuracy: 0.7876\n",
            "Epoch 19/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2252 - accuracy: 0.9022 - val_loss: 1.0305 - val_accuracy: 0.5133\n",
            "Epoch 20/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1593 - accuracy: 0.9267 - val_loss: 0.6539 - val_accuracy: 0.6814\n",
            "Epoch 21/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.2342 - accuracy: 0.8733 - val_loss: 0.8432 - val_accuracy: 0.6726\n",
            "Epoch 22/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2263 - accuracy: 0.8889 - val_loss: 0.4037 - val_accuracy: 0.8496\n",
            "Epoch 23/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.1983 - accuracy: 0.9089 - val_loss: 0.6815 - val_accuracy: 0.6726\n",
            "Epoch 24/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1814 - accuracy: 0.9267 - val_loss: 0.4089 - val_accuracy: 0.8584\n",
            "Epoch 25/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.1871 - accuracy: 0.9267 - val_loss: 0.5012 - val_accuracy: 0.7522\n",
            "Epoch 26/150\n",
            "15/15 [==============================] - 0s 21ms/step - loss: 0.2394 - accuracy: 0.8978 - val_loss: 0.6128 - val_accuracy: 0.6814\n",
            "Epoch 27/150\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 0.2227 - accuracy: 0.9044 - val_loss: 2.4874 - val_accuracy: 0.5929\n",
            "Epoch 28/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1941 - accuracy: 0.9200 - val_loss: 1.2184 - val_accuracy: 0.7168\n",
            "Epoch 29/150\n",
            "15/15 [==============================] - 0s 26ms/step - loss: 0.2966 - accuracy: 0.8756 - val_loss: 0.9121 - val_accuracy: 0.7080\n",
            "Epoch 30/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.2151 - accuracy: 0.9133 - val_loss: 0.7248 - val_accuracy: 0.7434\n",
            "Epoch 31/150\n",
            "15/15 [==============================] - 0s 23ms/step - loss: 0.1764 - accuracy: 0.9311 - val_loss: 0.5523 - val_accuracy: 0.7080\n",
            "Epoch 32/150\n",
            "15/15 [==============================] - 0s 24ms/step - loss: 0.1391 - accuracy: 0.9467 - val_loss: 0.4834 - val_accuracy: 0.7699\n",
            "Epoch 33/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.1285 - accuracy: 0.9511 - val_loss: 0.4815 - val_accuracy: 0.8496\n",
            "Epoch 34/150\n",
            "15/15 [==============================] - 0s 25ms/step - loss: 0.1243 - accuracy: 0.9511 - val_loss: 0.5729 - val_accuracy: 0.8053\n",
            "Epoch 35/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1228 - accuracy: 0.9533 - val_loss: 0.3973 - val_accuracy: 0.8230\n",
            "Epoch 36/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1449 - accuracy: 0.9311 - val_loss: 0.6589 - val_accuracy: 0.7611\n",
            "Epoch 37/150\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1320 - accuracy: 0.9511 - val_loss: 0.5751 - val_accuracy: 0.6991\n",
            "Epoch 38/150\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.1298 - accuracy: 0.9533 - val_loss: 1.7267 - val_accuracy: 0.5398\n",
            "Epoch 39/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.2420 - accuracy: 0.8867 - val_loss: 0.8937 - val_accuracy: 0.6372\n",
            "Epoch 40/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1293 - accuracy: 0.9511 - val_loss: 1.3544 - val_accuracy: 0.7257\n",
            "Epoch 41/150\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1153 - accuracy: 0.9578 - val_loss: 0.6399 - val_accuracy: 0.8319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Autoencoder(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    def Encoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        x = keras.layers.MaxPool1D(pool_size=2, padding=\"same\")(x)\n",
        "        return x\n",
        "\n",
        "    def Decoder(input):\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=32, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(input)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3, activation=\"relu\", padding=\"same\"\n",
        "        )(x)\n",
        "        # x = keras.layers.Conv1D(filters=64, kernel_size=2, activation=\"relu\")(x)\n",
        "        x = keras.layers.UpSampling1D(size=2)(x)\n",
        "        x = keras.layers.Conv1D(\n",
        "            filters=1, kernel_size=3, activation=\"linear\", padding=\"same\"\n",
        "        )(x)\n",
        "        return x\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input = keras.Input(shape=(n_timesteps,n_features))\n",
        "    autoencoder = keras.Model(inputs=orig_input, outputs=Decoder(Encoder(orig_input)))\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def AutoencoderLSTM(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    # structure from medium post: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
        "    def EncoderLSTM(input):\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(input)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(input)\n",
        "        # encoded = keras.layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "        encoded = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(x)\n",
        "        return encoded\n",
        "\n",
        "    def DecoderLSTM(encoded):\n",
        "        x = keras.layers.RepeatVector(n_timesteps)(encoded)\n",
        "        # x = keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True)(x)\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x)\n",
        "        decoded = keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(n_features, activation=\"sigmoid\")\n",
        "        )(x)\n",
        "        return decoded\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input2 = keras.Input(shape=( n_timesteps,n_features))\n",
        "\n",
        "    autoencoder2 = keras.Model(\n",
        "        inputs=orig_input2, outputs=DecoderLSTM(EncoderLSTM(orig_input2))\n",
        "    )\n",
        "\n",
        "    return autoencoder2\n"
      ],
      "metadata": {
        "id": "YopwiyfxDr_g"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded.shape"
      ],
      "metadata": {
        "id": "0kU7VTl9Ev-I",
        "outputId": "37266952-8334-4d39-c8a4-fbb3dc79d349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(450, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_processed_padded.shape"
      ],
      "metadata": {
        "id": "2LaZyeYcH6a4",
        "outputId": "62ec829e-f897-4e48-c8d5-ffff0556061d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(113, 896, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_processed_padded = np.transpose(X_train_processed_padded, (0, 2, 1))"
      ],
      "metadata": {
        "id": "AL7pGcHjIKvj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_processed_padded = np.transpose(X_test_processed_padded, (0, 2, 1))"
      ],
      "metadata": {
        "id": "zDaWXppWITcy"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "a6cbf655-b4d5-49a8-a3c0-f61a36f085fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 7s - loss: 0.0643 - val_loss: 0.0138 - 7s/epoch - 471ms/step\n",
            "Epoch 2/50\n",
            "15/15 - 3s - loss: 0.0045 - val_loss: 0.0013 - 3s/epoch - 192ms/step\n",
            "Epoch 3/50\n",
            "15/15 - 3s - loss: 0.0014 - val_loss: 0.0012 - 3s/epoch - 173ms/step\n",
            "Epoch 4/50\n",
            "15/15 - 2s - loss: 0.0011 - val_loss: 9.0219e-04 - 2s/epoch - 110ms/step\n",
            "Epoch 5/50\n",
            "15/15 - 2s - loss: 8.1441e-04 - val_loss: 6.6226e-04 - 2s/epoch - 109ms/step\n",
            "Epoch 6/50\n",
            "15/15 - 2s - loss: 6.2639e-04 - val_loss: 6.2277e-04 - 2s/epoch - 108ms/step\n",
            "Epoch 7/50\n",
            "15/15 - 2s - loss: 5.9381e-04 - val_loss: 5.9856e-04 - 2s/epoch - 107ms/step\n",
            "Epoch 8/50\n",
            "15/15 - 2s - loss: 5.7754e-04 - val_loss: 5.8973e-04 - 2s/epoch - 107ms/step\n",
            "Epoch 9/50\n",
            "15/15 - 2s - loss: 5.7019e-04 - val_loss: 5.8374e-04 - 2s/epoch - 118ms/step\n",
            "Epoch 10/50\n",
            "15/15 - 3s - loss: 5.6577e-04 - val_loss: 5.8037e-04 - 3s/epoch - 203ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.0005803668755106628.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoderLSTM = AutoencoderLSTM(n_timesteps_padded, n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoderLSTM.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoderLSTM.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_test_processed_padded, X_test_processed_padded),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")"
      ],
      "metadata": {
        "id": "C5Sk6xWIKxMd",
        "outputId": "c08026f2-afd6-4c05-c721-e7d3bd595e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "15/15 - 45s - loss: 0.0183 - val_loss: 0.0093 - 45s/epoch - 3s/step\n",
            "Epoch 2/50\n",
            "15/15 - 33s - loss: 0.0106 - val_loss: 0.0085 - 33s/epoch - 2s/step\n",
            "Epoch 3/50\n",
            "15/15 - 33s - loss: 0.0101 - val_loss: 0.0077 - 33s/epoch - 2s/step\n",
            "Epoch 4/50\n",
            "15/15 - 33s - loss: 0.0080 - val_loss: 0.0048 - 33s/epoch - 2s/step\n",
            "Epoch 5/50\n",
            "15/15 - 33s - loss: 0.0089 - val_loss: 0.0082 - 33s/epoch - 2s/step\n",
            "Epoch 6/50\n",
            "15/15 - 33s - loss: 0.0069 - val_loss: 0.0047 - 33s/epoch - 2s/step\n",
            "Epoch 7/50\n",
            "15/15 - 33s - loss: 0.0051 - val_loss: 0.0041 - 33s/epoch - 2s/step\n",
            "Epoch 8/50\n",
            "15/15 - 34s - loss: 0.0046 - val_loss: 0.0038 - 34s/epoch - 2s/step\n",
            "Epoch 9/50\n",
            "15/15 - 33s - loss: 0.0045 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 10/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0033 - 33s/epoch - 2s/step\n",
            "Epoch 11/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0060 - 33s/epoch - 2s/step\n",
            "Epoch 12/50\n",
            "15/15 - 34s - loss: 0.0055 - val_loss: 0.0042 - 34s/epoch - 2s/step\n",
            "Epoch 13/50\n",
            "15/15 - 34s - loss: 0.0045 - val_loss: 0.0038 - 34s/epoch - 2s/step\n",
            "Epoch 14/50\n",
            "15/15 - 33s - loss: 0.0040 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 15/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0033 - 33s/epoch - 2s/step\n",
            "Epoch 16/50\n",
            "15/15 - 34s - loss: 0.0038 - val_loss: 0.0035 - 34s/epoch - 2s/step\n",
            "Epoch 17/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 18/50\n",
            "15/15 - 35s - loss: 0.0035 - val_loss: 0.0033 - 35s/epoch - 2s/step\n",
            "Epoch 19/50\n",
            "15/15 - 33s - loss: 0.0038 - val_loss: 0.0042 - 33s/epoch - 2s/step\n",
            "Epoch 20/50\n",
            "15/15 - 33s - loss: 0.0053 - val_loss: 0.0039 - 33s/epoch - 2s/step\n",
            "Epoch 21/50\n",
            "15/15 - 33s - loss: 0.0043 - val_loss: 0.0034 - 33s/epoch - 2s/step\n",
            "Epoch 22/50\n",
            "15/15 - 33s - loss: 0.0042 - val_loss: 0.0035 - 33s/epoch - 2s/step\n",
            "Epoch 23/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0036 - 33s/epoch - 2s/step\n",
            "Epoch 24/50\n",
            "15/15 - 33s - loss: 0.0045 - val_loss: 0.0044 - 33s/epoch - 2s/step\n",
            "Epoch 25/50\n",
            "15/15 - 33s - loss: 0.0040 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 26/50\n",
            "15/15 - 33s - loss: 0.0037 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 27/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 28/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 29/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 30/50\n",
            "15/15 - 33s - loss: 0.0039 - val_loss: 0.0032 - 33s/epoch - 2s/step\n",
            "Epoch 31/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 32/50\n",
            "15/15 - 33s - loss: 0.0035 - val_loss: 0.0031 - 33s/epoch - 2s/step\n",
            "Epoch 33/50\n",
            "15/15 - 33s - loss: 0.0036 - val_loss: 0.0063 - 33s/epoch - 2s/step\n",
            "Epoch 34/50\n",
            "15/15 - 33s - loss: 0.0055 - val_loss: 0.0038 - 33s/epoch - 2s/step\n",
            "Epoch 35/50\n",
            "15/15 - 33s - loss: 0.0047 - val_loss: 0.0039 - 33s/epoch - 2s/step\n",
            "Epoch 36/50\n",
            "15/15 - 33s - loss: 0.0044 - val_loss: 0.0036 - 33s/epoch - 2s/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.0030726369004696608.\n"
          ]
        }
      ]
    }
  ]
}